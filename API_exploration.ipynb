{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4baa64",
   "metadata": {},
   "source": [
    "# API Functions to import/export/update eLab\n",
    "\n",
    "Here we will provide some examples of API functionalities with case examples\n",
    "\n",
    "## Configuration of eLab API and Google Drive API\n",
    "To install google-api to be able to query the table, see https://medium.com/swlh/google-drive-api-with-python-part-i-set-up-credentials-1f729cb0372b.\n",
    "\n",
    "Then to use it, you may be interested in that help: https://billydharmawan.medium.com/?p=e8c7b4b79f39\n",
    "\n",
    "Note that the actions to create the credentials.json file below do not work from jupyter. Just open a python shell and copy-paste them! Once this done, the code below works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ac0bae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import pandas\n",
    "import numpy\n",
    "from apiclient import discovery, errors\n",
    "from httplib2 import Http\n",
    "from oauth2client import client, file, tools\n",
    "import os.path\n",
    "\n",
    "token = format(open(\"credentials/tokenELAB\",\"r\").readline().strip())\n",
    "url = \"https://elab-dev.pasteur.fr/api/v1/\"\n",
    "headers1 = {'Authorization': token, 'Accept': 'application/json','Content-Type':'application/json'}\n",
    "headers2 = {'Authorization': token, 'Accept': 'application/json'}\n",
    "params={}\n",
    "\n",
    "\n",
    "credentials_file_path = './credentials/credentials.json'\n",
    "clientsecret_file_path = './credentials/client_secret.json'\n",
    "#print(os.path.isfile(clientsecret_file_path ))\n",
    "#print(os.path.isfile(credentials_file_path ))\n",
    "SCOPE = 'https://www.googleapis.com/auth/drive'\n",
    "\n",
    "store = file.Storage(credentials_file_path)\n",
    "credentials=store.get()\n",
    "if not credentials or credentials.invalid:\n",
    "    flow =  client.flow_from_clientsecrets(clientsecret_file_path, SCOPE)\n",
    "    print(flow.client_id)\n",
    "    credentials =  tools.run_flow(flow, store)\n",
    "\n",
    "http = credentials.authorize(Http())\n",
    "drive = discovery.build('drive','v3',http=http)\n",
    "sheets = discovery.build('sheets', 'v4', credentials=credentials)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa2163",
   "metadata": {},
   "source": [
    "## Class Definition\n",
    "Here we define classes for experiments, sample types etc\n",
    "\n",
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8c0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"experiments\", headers = headers2,params = params)\n",
    "data = r.json()\n",
    "experiments = {}\n",
    "for exp in data.get(\"data\"):\n",
    "    experiments[format(exp.get(\"name\"))] = format(exp.get(\"experimentID\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f7a2d",
   "metadata": {},
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a5ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"projects\", headers = headers2,params = params)\n",
    "data = r.json()\n",
    "projects = {}\n",
    "for pro in data.get(\"data\"):\n",
    "    projects[format(pro.get(\"name\"))] = format(pro.get(\"projectID\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1ed46",
   "metadata": {},
   "source": [
    "### Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe10142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'storageLayerID': 774661, 'storageType': {'storageTypeID': 2, 'groupID': 0, 'userID': 0, 'name': '-20 Freezer', 'deviceType': 'STORAGE'}, 'deviceType': 'STORAGE', 'deviceTypeID': 2, 'deviceTypeName': '-20 Freezer', 'barcode': '008000000774661', 'status': 'Available', 'storageID': 50002, 'instituteID': 631, 'groupID': 10684, 'userID': 40629, 'storageTypeID': 2, 'name': 'Freezer n9', 'department': 'The GLOBE Institute, Faculty of Health and Medical Sciences, University of Copenhagen', 'address': '', 'building': '', 'floor': '', 'room': '', 'notes': ''}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url + \"storage\", headers = headers2)\n",
    "data = r.json()\n",
    "storage = {}\n",
    "for sto in data.get(\"data\"):\n",
    "    storage[format(sto.get(\"name\"))] = format(sto.get(\"storageID\"))\n",
    "    if sto.get(\"name\") == \"Freezer n9\":\n",
    "        print(sto)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ee2571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'storageLayerID': 776238, 'storageType': {'storageTypeID': 4086, 'groupID': 10684, 'userID': 40629, 'name': 'Lugage', 'deviceType': 'STORAGE'}, 'deviceType': 'STORAGE', 'deviceTypeID': 4086, 'deviceTypeName': 'Lugage', 'barcode': '008000000776238', 'status': 'Available', 'storageID': 50157, 'instituteID': 631, 'groupID': 10684, 'userID': 40629, 'storageTypeID': 4086, 'name': 'With Lumila', 'department': '', 'address': '', 'building': '', 'floor': '', 'room': '', 'notes': ''}\n",
      "{'Nico office': '49999', 'In Copenhagen': '50000', 'Tom Gilbert Freezer': '50001', 'Freezer n9': '50002', 'Freezer 4': '50027', 'Unknown': '50028', 'Individual': '50056', 'Site': '50057', 'Sequencing': '50099', 'Hannes Freezer': '50122', 'In Tartu': '50135', 'With Lumila': '50157'}\n"
     ]
    }
   ],
   "source": [
    "print(sto)\n",
    "print(storage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b0e97",
   "metadata": {},
   "source": [
    "## Obtain classes for each all samples types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa5aeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Individual': '39466', 'Site': '39468', 'Skeleton Element': '39469', 'Extract': '39470', 'Indexed Library': '39494', 'Library pool': '39495', 'Non Indexed Library': '39556'}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url + \"sampleTypes\", headers = headers2)\n",
    "data = r.json()\n",
    "#types = []\n",
    "#for typ in data.get(\"data\"):\n",
    "#    types.append({format(typ.get(\"name\")):format(typ.get(\"sampleTypeID\"))})\n",
    "types = {}\n",
    "for typ in data.get(\"data\"):\n",
    "    types[format(typ.get(\"name\"))] = format(typ.get(\"sampleTypeID\"))\n",
    "\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857e69b",
   "metadata": {},
   "source": [
    "## Obtain the list of samples for each sample type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9455c444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AR0003': '9477522', 'AR0004': '9477523', 'AR0005': '9477524', 'AR0006': '9477525', 'AR0007': '9477526', 'AR0008': '9477527', 'AR0009': '9477528', 'AR0010': '9477529', 'AR0011': '9477530', 'AR0012': '9477531', 'AR0013': '9477532', 'AR0014': '9477533', 'AR0015': '9477534', 'AR0016': '9477535', 'AR0017': '9477536', 'AR0018': '9477537', 'AR0019': '9477538', 'AR0020': '9477539', 'AR0021': '9477540', 'AR0022': '9477541', 'AR0023': '9477542', 'AR0024': '9477543', 'AR0025': '9477544', 'AR0026': '9477545', 'AR0027': '9477546', 'AR0028': '9477547', 'AR0029': '9477548', 'AR0030': '9477549', 'AR0031': '9477550', 'AR0032': '9477551', 'AR0033': '9477552', 'AR0034': '9477553', 'AR0035': '9477554', 'AR0037': '9477555', 'AR0038': '9477556', 'AR0039': '9477557', 'AR0040': '9477558', 'AR0041': '9477559', 'AR0042': '9477560', 'AR0043': '9477561', 'AR0044': '9477562', 'AR0045': '9477563', 'AR0046': '9477564', 'AR0047': '9477565', 'AR0048': '9477566', 'AR0049': '9477567', 'AR0050': '9477568', 'AR0051': '9477569', 'AR0052': '9477570', 'AR0053': '9477571', 'AR0054': '9477572', 'AR0055': '9477573', 'AR0056': '9477574', 'AR0057': '9477575', 'AR0058': '9477576', 'AR0059': '9477577', 'AR0060': '9477578', 'AR0061': '9477579', 'AR0062': '9477580', 'AR0063': '9477581', 'AR0064': '9477582', 'AR0065': '9477583', 'AR0066': '9477584', 'AR0067': '9477585', 'AR0068': '9477586', 'AR0069': '9477587', 'AR0070': '9477588', 'AR0071': '9477589', 'AR0072': '9477590', 'AR0073': '9477591', 'AR0074': '9477592', 'AR0075': '9477593', 'AR0076': '9477594', 'AR0077': '9477595', 'AR0078': '9477596', 'AR0079': '9477597', 'AR0080': '9477598', 'AR0081': '9477599', 'AR0082': '9477600', 'AR0083': '9477601', 'AR0084': '9477602', 'AR0085': '9477603', 'AR0086': '9477604', 'AR0087': '9477605', 'AR0088': '9477606', 'AR0089': '9477607', 'AR0090': '9477608', 'AR0091': '9477609', 'AR0092': '9477610', 'AR0093': '9477611', 'AR0094': '9477612', 'AR0095': '9477613', 'AR0096': '9477614', 'AR0097': '9477615', 'AR0098': '9477616', 'AR0099': '9477617', 'AR0100': '9477618', 'AR0101': '9477619', 'AR0102': '9477620', 'AR0103': '9477621', 'AR0104': '9477622', 'AR0105': '9477623', 'AR0106': '9477624', 'AR0107': '9477625', 'AR0108': '9477626', 'AR0109': '9477627', 'AR0110': '9477628', 'AR0111': '9477629', 'AR0112': '9477630', 'AR0113': '9477631', 'AR0114': '9477632', 'AR0115': '9477633', 'AR0116': '9477634', 'AR0117': '9477635', 'AR0118': '9477636', 'AR0119': '9477637', 'AR0120': '9477638', 'AR0121': '9477639', 'AR0122': '9477640', 'AR0123': '9477641', 'AR0124': '9477642', 'AR0125': '9477643', 'AR0126': '9477644', 'AR0127': '9477645', 'AR0128': '9477646', 'AR0129': '9477647', 'AR0130': '9477648', 'AR0131': '9477649', 'AR0132': '9477650', 'AR0133': '9477651', 'AR0134': '9477652', 'AR0135': '9477653', 'AR0136': '9477654', 'AR0137': '9477655', 'AR0138': '9477656', 'AR0139': '9477657', 'AR0140': '9477658', 'AR0141': '9477659', 'AR0142': '9477660', 'AR0143': '9477661', 'AR0144': '9477662', 'AR0145': '9477663', 'AR0146': '9477664', 'AR0147': '9477665', 'AR0148': '9477666', 'AR0149': '9477667', 'AR0150': '9477668', 'AR0151': '9477669', 'AR0152': '9477670', 'AR0153': '9477671', 'AR0154': '9477672', 'AR0155': '9477673', 'AR0156': '9477674', 'AR0157': '9477675', 'AR0158': '9477676', 'AR0159': '9477677', 'AR0160': '9477678', 'AR0161': '9477679', 'AR0162': '9477680', 'AR0163': '9477681', 'AR0164': '9477682', 'AR0165': '9477683', 'AR0166': '9477684', 'AR0167': '9477685', 'AR0168': '9477686', 'AR0169': '9477687', 'AR0170': '9477688', 'AR0171': '9477689', 'AR0172': '9477690', 'AR0173': '9477691', 'AR0174': '9477692', 'AR0175': '9477693', 'AR0176': '9477694', 'AR0177': '9477695', 'AR0178': '9477696', 'AR0179': '9477697', 'AR0180': '9477698', 'AR0181': '9477699', 'AR0182': '9477700', 'AR0183': '9477701', 'AR0184': '9477702', 'AR0185': '9477703', 'AR0186': '9477704', 'AR0187': '9477705', 'AR0188': '9477706', 'AR0189': '9481969', 'AR0190': '9481970', 'AR0191': '9481971', 'AR0192': '9481972', 'AR0193': '9481973', 'AR0194': '9481974', 'AR0195': '9481975', 'AR0196': '9481976', 'AR0197': '9481977', 'AR0198': '9481978', 'AR0199': '9481979', 'AR0200': '9481980', 'AR0201': '9481981', 'AR0202': '9481982', 'AR0203': '9481983', 'AR0204': '9481984', 'AR0205': '9481985', 'AR0206': '9481986', 'AR0207': '9481987', 'AR0208': '9481988', 'AR0209': '9481989', 'AR0210': '9481990', 'AR0211': '9481991', 'AR0212': '9481992', 'AR0213': '9481993', 'AR0214': '9481994', 'AR0215': '9481995', 'AR0216': '9481996', 'AR0217': '9481997', 'AR0218': '9481998', 'AR0219': '9481999', 'AR0220': '9482000', 'AR0221': '9482001', 'AR0222': '9482002', 'AR0223': '9482003', 'AR0224': '9482004', 'AR0225': '9482005', 'AR0226': '9482006', 'AR0227': '9482007', 'AR0228': '9482008', 'AR0229': '9482009', 'AR0230': '9482010', 'AR0231': '9482011', 'AR0232': '9482012', 'AR0233': '9482013', 'AR0234': '9482014', 'AR0235': '9482015', 'AR0236': '9482016', 'AR0237': '9482017', 'AR0238': '9482018', 'AR0239': '9482019', 'AR0240': '9482020', 'AR0241': '9482021', 'AR0242': '9482022', 'AR0243': '9482023', 'AR0244': '9482024', 'AR0245': '9482025', 'AR0246': '9482026', 'AR0247': '9482027', 'AR0248': '9482028', 'AR0249': '9482029', 'AR0250': '9482030', 'AR0251': '9482031', 'AR0252': '9482032', 'AR0253': '9482033', 'AR0254': '9482034', 'AR0255': '9482035', 'AR0256': '9482036', 'AR0257': '9482037', 'AR0258': '9482038', 'AR0259': '9482039', 'AR0260': '9482040', 'AR0261': '9482041', 'AR0262': '9482042', 'AR0263': '9482043', 'AR0264': '9482044', 'AR0265': '9482045', 'AR0266': '9482046', 'AR0267': '9482047', 'AR0268': '9482048', 'AR0269': '9482049', 'AR0270': '9482050', 'AR0271': '9482051', 'AR0272': '9482052', 'AR0273': '9482053', 'AR0274': '9482054', 'AR0275': '9482055', 'AR0276': '9482056', 'AR0277': '9482057', 'AR0278': '9482058', 'AR0279': '9482059', 'AR0281': '9482062', 'AR0282': '9482063', 'AR0283': '9482064', 'AR0284': '9482065', 'AR0285': '9482066', 'AR0286': '9482067', 'AR0287': '9482068', 'AR0288': '9482069', 'AR0289': '9482070', 'AR0290': '9482071', 'AR0291': '9482072', 'AR0292': '9482073', 'AR0293': '9482074', 'AR0294': '9482075', 'AR0295': '9482076', 'AR0296': '9482077', 'AR0297': '9482078', 'AR0299': '9482081', 'AR0300': '9482082', 'AR0301': '9482083', 'AR0302': '9482084', 'AR0303': '9482085', 'AR0304': '9482086', 'AR0305': '9482087', 'AR0306': '9482088', 'AR0307': '9482089', 'AR0308': '9482090', 'AR0309': '9482091', 'AR0310': '9482092', 'AR0311': '9482093', 'AR0312': '9482094', 'AR0313': '9482095', 'AR0314': '9482096', 'AR0315': '9482097', 'AR0316': '9482098', 'AR0317': '9482099', 'AR0318': '9482100', 'AR0319': '9482101', 'AR0320': '9482102', 'AR0321': '9482103', 'AR0322': '9482105', 'AR0323': '9482106', 'AR0324': '9482107', 'AR0325': '9482108', 'AR0326': '9482109', 'AR0327': '9482110', 'AR0328': '9482111', 'AR0329': '9482112', 'AR0330': '9482113', 'AR0331': '9482114', 'AR0332': '9482115', 'AR0333': '9482116', 'AR0334': '9482117', 'AR0335': '9482118', 'AR0336': '9482119', 'AR0337': '9482120', 'AR0338': '9482121', 'AR0339': '9482122', 'AR0340': '9482123', 'AR0341': '9482124', 'AR0342': '9482125', 'AR0343': '9482126', 'AR0344': '9482127', 'AR0345': '9482128', 'AR0346': '9482129', 'AR0347': '9482130', 'AR0348': '9482131', 'AR0349': '9482132', 'AR0350': '9482133', 'AR0351': '9482134', 'AR0352': '9482135', 'AR0353': '9482136', 'AR0354': '9482137', 'AR0355': '9482138', 'AR0356': '9482140', 'AR0357': '9482142', 'AR0358': '9482143', 'AR0359': '9482144', 'AR0360': '9482145', 'AR0361': '9482146', 'AR0362': '9482147', 'AR0363': '9482148', 'AR0364': '9482149', 'AR0365': '9482150', 'AR0366': '9482151', 'AR0367': '9482152', 'AR0368': '9482153', 'AR0369': '9482154', 'AR0370': '9482155', 'AR0371': '9482156', 'AR0001': '9507924', 'AR0002': '9507925', 'AR0280': '9519612', 'AR0298': '9519613', 'AR0372': '9519614', 'AR0373': '9519615', 'AR0374': '9519616', 'AR0375': '9519617', 'AR0376': '9519618', 'AR0377': '9519619', 'AR0378': '9519620', 'AR0379': '9519621', 'AR0380': '9519622', 'AR0381': '9519623', 'AR0383': '9519625', 'AR0387': '9519629', 'AR0388': '9519630', 'AR0389': '9519631', 'AR0390': '9519632', 'AR0391': '9519633', 'AR0392': '9519634', 'AR0393': '9519635', 'AR0394': '9519636', 'AR0395': '9519637', 'AR0396': '9519638', 'AR0397': '9519639', 'AR0398': '9519640', 'AR0399': '9519641', 'AR0400': '9519642', 'AR0401': '9519643', 'AR0402': '9519644', 'AR0403': '9519645', 'AR0404': '9519646', 'AR0405': '9519647', 'AR0406': '9519648', 'AR0407': '9519649', 'AR0408': '9519650', 'AR0409': '9519651', 'AR0410': '9519652', 'AR0411': '9519653', 'AR0412': '9519654', 'AR0413': '9519655', 'AR0414': '9519656', 'AR0415': '9519657', 'AR0416': '9519658', 'AR0417': '9519659', 'AR0418': '9519660', 'AR0419': '9519661', 'AR0420': '9519662', 'AR0421': '9519663', 'AR0422': '9519664', 'AR0423': '9519665', 'AR0424': '9519666', 'AR0425': '9519667', 'AR0426': '9519668', 'AR0427': '9519669', 'AR0428': '9519670', 'AR0429': '9519671', 'AR0430': '9519672', 'AR0431': '9519673', 'AR0432': '9519674', 'AR0433': '9519675', 'AR0434': '9519676', 'AR0435': '9519677', 'AR0436': '9519678', 'AR0437': '9519679', 'AR0438': '9519680', 'AR0439': '9519681', 'AR0440': '9519682', 'AR0441': '9519683', 'AR0442': '9519684', 'AR0443': '9519685', 'AR0444': '9519686', 'AR0445': '9519687', 'AR0446': '9519688', 'AR0447': '9519689', 'AR0448': '9519690', 'AR0449': '9519691', 'AR0450': '9519692', 'AR0451': '9519693', 'AR0452': '9519694', 'AR0453': '9519695', 'AR0454': '9519696', 'AR0455': '9519697', 'AR0456': '9519698', 'AR0457': '9519699', 'AR0458': '9519700', 'AR0459': '9519701', 'AR0460': '9519702', 'AR0461': '9519703', 'AR0462': '9519704', 'AR0463': '9519705', 'AR0464': '9519706', 'AR0465': '9519707', 'AR0466': '9519708', 'AR0467': '9519709', 'AR0468': '9519710', 'AR0469': '9519711', 'AR0470': '9519712', 'AR0471': '9519713', 'AR0472': '9519714', 'AR0473': '9519715', 'AR0474': '9519716', 'AR0475': '9519717', 'AR0478': '9519720', 'AR0479': '9519721', 'AR0480': '9519722', 'AR0481': '9519723', 'AR0482': '9519724', 'AR0483': '9519725', 'AR0484': '9519726', 'AR0485': '9519727', 'AR0486': '9519728', 'AR0487': '9519729', 'AR0488': '9519730', 'AR0489': '9519731', 'AR0490': '9519732', 'AR0491': '9519733', 'AR0492': '9519734', 'AR0493': '9519735', 'AR0494': '9519736', 'AR0495': '9519737', 'AR0496': '9519738', 'AR0497': '9519739', 'AR0498': '9519740', 'AR0499': '9519741', 'AR0500': '9519742', 'AR0501': '9519743', 'AR0502': '9519744', 'AR0503': '9519745', 'AR0504': '9519746', 'AR0505': '9519747', 'AR0506': '9519748', 'AR0507': '9519749', 'AR0508': '9519750', 'AR0509': '9519751', 'AR0510': '9519752', 'AR0511': '9519753', 'AR0512': '9519754', 'AR0513': '9519755', 'AR0514': '9519756', 'AR0515': '9519757', 'AR0516': '9519758', 'AR0517': '9519759', 'AR0518': '9519760', 'AR0519': '9519761', 'AR0520': '9519762', 'AR0521': '9519763', 'AR0522': '9519764', 'AR0523': '9519765', 'AR0524': '9519766', 'AR0526': '9519768', 'AR0527': '9519769', 'AR0528': '9519770', 'AR0529': '9519771', 'AR0530': '9519772', 'AR0531': '9519773', 'AR0532': '9519774', 'AR0533': '9519775', 'AR0534': '9519776', 'AR0535': '9519777', 'AR0536': '9519778', 'AR0537': '9519779', 'AR0538': '9519780', 'AR0539': '9519781', 'AR0540': '9519782', 'AR0541': '9519783', 'AR0542': '9519784', 'AR0543': '9519785', 'AR0544': '9519786', 'AR0545': '9519787', 'AR0546': '9519788', 'AR0547': '9519789', 'AR0548': '9519790', 'AR0549': '9519791', 'AR0550': '9519792', 'AR0551': '9519793', 'AR0552': '9519794', 'AR0553': '9519795', 'AR0554': '9519796', 'AR0555': '9519797', 'AR0556': '9519798', 'AR0557': '9519799', 'AR0558': '9519800', 'AR0559': '9519801', 'AR0560': '9519802', 'AR0561': '9519803', 'AR0562': '9519804', 'AR0563': '9519805', 'AR0564': '9519806', 'AR0565': '9519807', 'AR0566': '9519808', 'AR0567': '9519809', 'AR0568': '9519810', 'AR0569': '9519811', 'AR0570': '9519812', 'AR0571': '9519813', 'AR0572': '9519814', 'AR0573': '9519815', 'AR0574': '9519816', 'AR0575': '9519817', 'AR0576': '9519818', 'AR0577': '9519819', 'AR0578': '9519820', 'AR0579': '9519821', 'AR0580': '9519822', 'AR0581': '9519823', 'AR0582': '9519824', 'AR0583': '9519825', 'AR0584': '9519826', 'AR0585': '9519827', 'AR0586': '9519828', 'AR0587': '9519829', 'AR0588': '9519830', 'AR0589': '9519831', 'AR0590': '9519832', 'AR0591': '9519833', 'AR0592': '9519834', 'AR0593': '9519835', 'AR0594': '9519836', 'AR0595': '9519837', 'AR0596': '9519838', 'AR0597': '9519839', 'AR0598': '9519840', 'AR0599': '9519841', 'AR0600': '9519842', 'AR0601': '9519843', 'AR0602': '9519844', 'AR0603': '9519845', 'AR0604': '9519846', 'AR0605': '9519847', 'AR0606': '9519848', 'AR0607': '9519849', 'AR0608': '9519850', 'AR0609': '9519851', 'AR0610': '9519852', 'AR0611': '9519853', 'AR0612': '9519854', 'AR0613': '9519855', 'AR0614': '9519856', 'AR0615': '9519857', 'AR0616': '9519858', 'AR0617': '9519859', 'AR0618': '9519860', 'AR0619': '9519861', 'AR0620': '9519862', 'AR0621': '9519863', 'AR0622': '9519864', 'AR0623': '9519865', 'AR0624': '9519866', 'AR0625': '9519867', 'AR0626': '9519868', 'AR0627': '9519869', 'AR0628': '9519870', 'AR0629': '9519871', 'AR0630': '9519872', 'AR0631': '9519873', 'AR0632': '9519874', 'AR0633': '9519875', 'AR0634': '9519876', 'AR0635': '9519877', 'AR0636': '9519878', 'AR0637': '9519879', 'AR0638': '9519880', 'AR0639': '9519881', 'AR0670': '9519882', 'AR0671': '9519883', 'AR0672': '9519884', 'AR0673': '9519885', 'AR0674': '9519886', 'AR0675': '9519887', 'AR0676': '9519888', 'AR0677': '9519889', 'AR0678': '9519890', 'AR0679': '9519891', 'AR0680': '9519892', 'AR0681': '9519893', 'AR0682': '9519894', 'AR0683': '9519895', 'AR0684': '9519896', 'AR0685': '9519897', 'AR0686': '9519898', 'AR0687': '9519899', 'AR0688': '9519900', 'AR0689': '9519901', 'AR0690': '9519902', 'AR0691': '9519903', 'AR0692': '9519904', 'AR0693': '9519905', 'AR0694': '9519906', 'AR0695': '9519907', 'AR0696': '9519908', 'AR0036': '9520970', 'AR0382': '9520971', 'AR0384': '9520972', 'AR0385': '9520973', 'AR0386': '9520974', 'AR0525': '9520975', 'AR0640': '9520976', 'AR0641': '9520977'}\n"
     ]
    }
   ],
   "source": [
    "registered = {}\n",
    "for it in types.items():\n",
    "    name = it[0]\n",
    "    ID = it[1]\n",
    "    #print(name + \" --> \" + ID)\n",
    "    r = requests.get(url + \"samples\" , headers = headers2, params = {'sampleTypeID': ID})\n",
    "    data = r.json()\n",
    "    myList = {}\n",
    "    for sam in data.get(\"data\"):\n",
    "        if format(sam.get(\"name\")) in myList.keys():\n",
    "            print(name + \": \" + sam.get(\"name\") + \" duplicated\")\n",
    "            break\n",
    "        myList[format(sam.get(\"name\"))]=format(sam.get(\"sampleID\"))\n",
    "    registered[name] = myList\n",
    "    \n",
    "print(registered[\"Individual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b822b99",
   "metadata": {},
   "source": [
    "## Upload samples to eLab\n",
    "\n",
    "### Define a dictionnary for feature names in eLab and in our tables\n",
    "One dictionary for each sample type.\n",
    "Note that \"parent sample\" is not a pre-set feature so it does not appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ac771",
   "metadata": {},
   "source": [
    "#### Get the columns corresponding to eLab features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a3df1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SiteDict={\n",
    "    \"Name\":\"Site\",\n",
    "    \"Description\":\"None\",\n",
    "    \"Note\":\"None\",\n",
    "    \"Amount\":\"fixed_1\",\n",
    "    #\"Unit\":\"fixed_Unit | pcs\",\n",
    "    \"Unit\":\"fixed_unit\",\n",
    "    \"Main geographic region\":\"Geographic Zone\",\n",
    "    \"Country\":\"Country\",\n",
    "    \"Province / Region\":\"Province / Region\",\n",
    "    \"Locality\":\"Locality\",\n",
    "    \"Latitude\":\"LatChanged\",\n",
    "    \"Longitude\":\"LongChanged\",\n",
    "    \"Site type\":\"Site type\",\n",
    "    \"Pictures\":\"None\"\n",
    "}\n",
    "\n",
    "IndDict={\n",
    "    \"Name\":\"RascovanLabID\",\n",
    "    \"Description\":\"None\",\n",
    "    \"Note\":\"None\",\n",
    "    \"Amount\":\"fixed_1\",\n",
    "    #\"Unit\":\"fixed_Unit | pcs\",\n",
    "    \"Unit\":\"fixed_unit\",\n",
    "    \"parentSampleID\":\"Site\",\n",
    "    \"Archaeologist ID\":\"Individual ID\",\n",
    "    \"Archaeologist group\":\"Archaeologists Group\",\n",
    "    \"Site Name\":\"Site\",\n",
    "    \"Date\":\"Date\",\n",
    "    \"Datation method\":\"Datation Method\",\n",
    "    #\"Subsistence Strategy\": \"Subsistence.Strategy\",\n",
    "    \"Age\":\"Age\",\n",
    "    \"Gender\":\"Gender\",\n",
    "    \"Pictures\":\"None\",\n",
    "    \"Linked individuals\":\"None\"\n",
    "}\n",
    "\n",
    "SkeDict={\"Name\":\"RascovanLabID\",\n",
    "         \"From Individual\":\"RascovanLabID\",\n",
    "         \"Description\":\"Observations\",\n",
    "         \"Note\":\"None\",\n",
    "         \"Amount\":\"fixed_1\",\n",
    "         #\"Unit\":\"fixed_Unit | pcs\",\n",
    "         \"Unit\":\"fixed_unit\",\n",
    "         \"parentSampleID\":\"TobeextractFromRascovanLabID\",\n",
    "         \"Archaeologist sample ID\":\"Sample ID\",\n",
    "         \"Pictures Labelling\":\"PicturePath\",\n",
    "         \"Bone type\":\"Bone Type\",\n",
    "         \"Skeleton element\":\"Skeletal Element\",\n",
    "         \"Exportation Permit Number\":\"Expediente\",\n",
    "         \"Observation Labelling\":\"Observation Pierre / Maria\",\n",
    "         \"Observation Drilling\":\"GeneralSampleComment\",\n",
    "         \"Pictures Drilling\":\"DrillingPictures\"\n",
    "}\n",
    "\n",
    "ExeDict={\"Name\":\"ExtractID\",\n",
    "         \"From Skeleton Element\":\"RascovanLabID\",\n",
    "         \"Description\":\"None\",\n",
    "         \"Note\":\"None\",\n",
    "         \"Amount\":\"Weight\",\n",
    "         \"Unit\":\"fixed_gram\",\n",
    "         \"parentSampleID\":\"RascovanLabID\",\n",
    "         \"Date of drilling\":\"Date\",\n",
    "         \"Pictures\":\"None\",\n",
    "         \"Person in charge\":\"fixed_Maria Lopopolo\",\n",
    "         \"Laboratory where processed\":\"fixed_Hannes Schroeder\",\n",
    "         \"Extract Type\":\"ExtractType\",\n",
    "         \"Conservation\":\"Observation\",\n",
    "         \"Pathology\":\"Pathologie\",\n",
    "         \"Pathology description\":\"None\",\n",
    "         \"Taken for extraction\":\"TakenForExtraction\",\n",
    "         \"Extracted\":\"Extraction\",\n",
    "         \"Extraction Comment\":\"extractionComment\",\n",
    "         \"density UDG treatment (ng/uL)\":\"densityUDGtreated\",\n",
    "         \"Volume UDG treatment (uL)\":\"volumeUDGtreated\",\n",
    "         \"mass UDG in Tube (ng)\":\"massInTube\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad1ce6",
   "metadata": {},
   "source": [
    "### For \"macro\" sample types (Site, Individual, Skeleton Element)\n",
    "\n",
    "We start from a tsv file (downloaded from there: https://docs.google.com/spreadsheets/d/1bnu9oZV5fXOaPY_KDvBEBSIIzbydPxyWdLg-A83cJnc/edit#gid=159434896 and then formatted through a Rscript...). (I will figure out how to download it automatically later), and then we register the site one by one. If a site exists in eLab, we change (patch) the values, if not we register it from scratch.\n",
    "\n",
    "To install google-api to be able to query the table, see https://medium.com/swlh/google-drive-api-with-python-part-i-set-up-credentials-1f729cb0372b.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1548d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded WholeDataSet.tsv\n"
     ]
    }
   ],
   "source": [
    "page_token = None\n",
    "driveFiles=[]\n",
    "while True:\n",
    "    try:\n",
    "        param = {}\n",
    "        if page_token:\n",
    "            param['pageToken'] = page_token\n",
    "        files = drive.files().list(**param).execute()\n",
    "        # append the files from the current result page to our list\n",
    "        driveFiles.extend(files.get('files'))\n",
    "        # Google Drive API shows our files in multiple pages when the number of files exceed 100\n",
    "        page_token = files.get('nextPageToken')\n",
    "        if not page_token:\n",
    "            break\n",
    "    except errors.HttpError as error:\n",
    "        print(f'An error has occurred: {error}')\n",
    "        break    # output the file metadata to console\n",
    "\n",
    "\n",
    "# define a function to export sheet to csv\n",
    "def download_sheet_to_csv(spreadsheet_id, sheet_name):\n",
    "    result = sheets.spreadsheets().values().get(spreadsheetId=spreadsheet_id, range=sheet_name).execute()\n",
    "    output_file = f'{sheet_name}.tsv'\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        writer = csv.writer(f,delimiter=\"\\t\")\n",
    "        writer.writerows(result.get('values'))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print(f'Successfully downloaded {sheet_name}.tsv')        \n",
    "        \n",
    "        \n",
    "        \n",
    "for file in driveFiles:\n",
    "    if file.get('name') == \"Conjuntos_Muestras_aDNA\":\n",
    "        id=file.get('id')\n",
    "        sheet='WholeDataSet'\n",
    "        download_sheet_to_csv(id,sheet)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db484d",
   "metadata": {},
   "source": [
    "Now read the table and format it!\n",
    "\n",
    "WATCHOUT! There are many things that will need some tuning as we add entries in the google spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a89b74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawTab=pandas.read_csv(\"WholeDataSet.tsv\",delimiter=\"\\t\")\n",
    "rawTab=rawTab.dropna(subset=['RascovanLabID'])\n",
    "rawTab.loc[rawTab['Latitude'].isnull(),\"Latitude\"]=None\n",
    "rawTab.loc[rawTab['Longitud'].isnull(),\"Longitud\"]=None\n",
    "rawTab.loc[rawTab['Latitude'].isin([ \"Undefined\",\"desconocido\",\"nan\"]),\"Latitude\"]=None\n",
    "rawTab.loc[rawTab['Longitud'].isin([ \"Undefined\",\"desconocido\",\"nan\"]),\"Longitud\"]=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33166d49",
   "metadata": {},
   "source": [
    "Change the coordinates so they are numerical (case by case here...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8554fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LatChanged  LongChanged\n",
      "0    -31.255803   -60.456464\n",
      "1    -31.255803   -60.456464\n",
      "2    -31.255803   -60.456464\n",
      "3    -31.255803   -60.456464\n",
      "4    -31.255803   -60.456464\n",
      "..          ...          ...\n",
      "867  -35.280000   -69.520000\n",
      "868  -35.150000   -69.650000\n",
      "869  -35.150000   -69.650000\n",
      "870  -35.150000   -69.650000\n",
      "871  -33.710000   -68.980000\n",
      "\n",
      "[871 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "###change coordinates\n",
    "def changeCOORfunction(coord):\n",
    "    stuf=\"º\"\n",
    "    stuf2=\"°\"\n",
    "    if coord is None:\n",
    "        #return([None,None,None,None])\n",
    "        return(None)\n",
    "\n",
    "    coordFed=coord\n",
    "    #print(coord)\n",
    "    #####HERE CHANGE ALL WEIRD CHARACTERS THAT CAN COME ON THE WAY\n",
    "    coord=coord.replace(\"39° 06 ́\",\"39°06'\")\n",
    "    coord=coord.replace(\"63° 47 ́\",\"63°47'\") \n",
    "    ##change weird degrees characters\n",
    "    coord=coord.replace(stuf2,stuf)\n",
    "    ##change weird minutes characters\n",
    "    coord=coord.replace(\"``\",\"\\\"\")\n",
    "    coord=coord.replace(\"´´\",\"\\\"\")\n",
    "    coord=coord.replace(\"”\",\"\\\"\")\n",
    "    coord=coord.replace(\"''\",\"\\\"\")\n",
    "    coord=coord.replace(\"’’\",\"\\\"\")\n",
    "    coord=coord.replace(\"“\",\"\\\"\")\n",
    "    ##change weird seconds characters\n",
    "    coord=coord.replace(\" ´\",\"'\")\n",
    "    coord=coord.replace(\"`\",\"'\")\n",
    "    coord=coord.replace(\"´\",\"'\")\n",
    "    coord=coord.replace(\"’\",\"'\")\n",
    "    ###we now that it is all South and West\n",
    "    coord=coord.replace(\" \",\"\")\n",
    "    coord=coord.replace(\"S\",\"\")\n",
    "    coord=coord.replace(\"O\",\"\")\n",
    "    coord=coord.replace(\"W\",\"\")\n",
    "    ###change decimal character\n",
    "    coord=coord.replace(\",\",\".\")\n",
    "\n",
    "    ##read degrees\n",
    "    if len(coord.split(stuf)) ==2 :\n",
    "        if coord.split(stuf)[1] == \"\":\n",
    "            if stuf not in coord:\n",
    "                coord=coord + stuf + \"0'\"\n",
    "            else:\n",
    "                coord=coord+\"0'\"\n",
    "    elif len(coord.split(stuf)) == 1 :\n",
    "        if stuf not in coord:\n",
    "            coord=coord + stuf + \"0'\"\n",
    "        else:\n",
    "            coord=coord+\"0'\"\n",
    "    else:\n",
    "        print(\"splitting minute/second \" + coordFed + \"-->\" + coord)\n",
    "            \n",
    "    deg=coord.split(stuf)[0]\n",
    "    ###read minutes and seconds\n",
    "    tmp=coord.split(stuf)[1]\n",
    "    minute=tmp.split(\"'\")[0]\n",
    "    if len(tmp.split(\"'\")) != 2:\n",
    "        print( coord.split(stuf))\n",
    "        print(\"splitting minute/second \" + coordFed + \"-->\" + coord)\n",
    "        raise()\n",
    "    else:\n",
    "        if tmp.split(\"'\")[1] == \"\":\n",
    "            sec=0\n",
    "        else:\n",
    "            sec=tmp.split(\"'\")[1]\n",
    "            sec=sec.replace(\"\\\"\",\"\")\n",
    "\n",
    "    #print([coordFed,coord,deg,minute,sec])\n",
    "    ####verify all read ok!\n",
    "    if numpy.isnan(float(deg)):\n",
    "        print(\"pb numerical degree\" + coordFed + \"-->\" +coord + \" (\" + deg + \")\")\n",
    "        raise()\n",
    "    if numpy.isnan(float(minute)):\n",
    "        print(minute)\n",
    "        print(\"pb numerical minute\" + coordFed + \"-->\" +coord + \" (\" + minute + \")\")\n",
    "        raise()\n",
    "    if numpy.isnan(float(sec)):\n",
    "        print(sec)\n",
    "        print(\"pb numerical sec\" + coordFed + \"-->\" + coord + \" (\" + sec + \")\")\n",
    "        raise()\n",
    "  \n",
    "    deg=float(deg)\n",
    "    minute=float(minute)\n",
    "    sec=float(sec)\n",
    "    new=deg+minute/60+sec/3600\n",
    "    #return([-new,deg,minute,sec])\n",
    "    return(-new)\n",
    "\n",
    "\n",
    "\n",
    "#print(rawTab.loc[rawTab['Latitude']==\"\"])\n",
    "rawTab['LatChanged']=rawTab['Latitude'].apply(changeCOORfunction)\n",
    "rawTab['LongChanged']=rawTab['Longitud'].apply(changeCOORfunction)\n",
    "\n",
    "print(rawTab[[\"LatChanged\",\"LongChanged\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73b39c",
   "metadata": {},
   "source": [
    "\n",
    "Make Bone Type variable according to Skeletal Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d66b0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dental Calculus\n",
      "['Dental Calculus']\n",
      "Tooth\n",
      "['Tooth (lower left second premolar)', 'Tooth (lower right first premolar)', 'Tooth (upper left first incisor)', 'Diente', 'M2i Md', 'Incisor', 'Tooth (lower right second incisor)', '1o premolar inferior derecho', 'I1dMx', 'Ci Mx', 'Tooth (second upper left premolar)', '2 molar superior izquierdo', 'first left superior molar', '3er molar inf izq', 'Tooth (second upper left incisor)', '3o molar superior izquierdo', '2° molar inferior izquierdo', 'I2d Md', 'PM1d Md', 'canino superior derecho', '2o molar inferior izquierdo', 'Premolar', 'Tooth', 'PM1d Mx', 'M2d Md', '2do molar inf der', 'I2 Md', 'PM2i Mx', 'Ci Md', 'Tooth (second upper right premolar, in pieces)', 'I1d Mx', '1 molar superior derecho', 'Tooth (upper left canine)', 'Tooth fragment', 'Tooth (lower left first premolar)', 'M3i Mx', 'Tooth (upper left second molar )', '3 molar inferior izquierdo', '3o molar superior derecho', 'molar', 'inferior molar', '1 molar inferior izquierdo', 'I1i Md', 'premolar superior derecho', 'incisivo central superior derecho', 'canino superior izquierdo', 'M2d Mx', 'incisivo lateral superior derecho', 'Tooth (right upper canine)', '2do molar inf izq', 'PM2i Md', 'I1d Md', 'Canine', 'PM2d Md', 'Tooth (upper right first premolar)', 'I1 Md', 'premolar superior izquiero', '2o premolar inferior izquierdo deciduo', 'Tooth (lower right second premolar)', 'Tooth (second upper right premolar)', '1 Molar Humano', 'M2i Mx', '1o premolar superio derecho', '3 molar superior derecho', 'incisivo lateral inferior derecho', 'M', 'premolar inferior izquierdo', '1o premolar superior derecho', 'Tooth (lower left canine)', 'Tooth (lower right first molar)', 'Tooth (second upper right incisor)', '2 molar inferior derecho', 'Cd Mx', 'Incisivo', 'M3d Mx', 'M3i Md', 'Tooth (lower left second incisor)', 'Cd Md', '1o molar inferior izquierdo', 'Molar', 'C Mx', '1o premolar inferior izquierdo', 'M3 Mx']\n",
      "Undefined\n",
      "['??']\n",
      "Other Bone\n",
      "['Longbone', 'Metacarpal OR vertebral hemiarch', 'Femur', 'Vertebra', 'Femur derecho. Molido', 'Tibia izquierda. Molido', '1 Fragmento de Craneo Humano', 'Bone (fibula)', 'femur fragment ', '1st lumbar', 'Metacarpo', 'Rib with pathology', '9th left rib', 'Bone (foot phalanx)', 'Metatarsus', 'humerus with patology', 'Hand phalange', 'Bone (radio)', 'Proc. matoideo', 'Vertebra ', 'Bone (metatarsal)', '9th right rib', 'Foot phalange', 'proximal phalange', 'Tarso', 'Bone (tibia)', 'Rib', 'Humerus']\n",
      "non Human\n",
      "['Valva']\n",
      "Petrous\n",
      "['petroso derecho', 'Petrous (right)', 'petroso', 'Petroso', 'petroso izquierdo', 'Petrous (left)', 'Petrous ', 'Petrous (left or right?)', 'Petrous?', 'Petrosos Derecho', 'Petrozo izquierdo', 'Petrous right', 'Petrous']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "l1=list(range(1,9)) * 8\n",
    "l2=list(itertools.chain.from_iterable(itertools.repeat(x, 8) for x in list(range(1,9))))\n",
    "l3=[]\n",
    "for i in list(range(0,len(l1))):\n",
    "    l3.append(str(l1[i])+\".\"+str(l2[i]))\n",
    "\n",
    "    \n",
    "toothElements=[\"tooth\",\n",
    "               \"diente\",\n",
    "               \"molar\",\n",
    "               \"mx\",\n",
    "               \"42 inf. 1° derecho\",\n",
    "               \" m \",\n",
    "               \" md\",\n",
    "               \"incisive\",\n",
    "               \"incisivo\",\n",
    "               \"incisor\",\n",
    "               \"canino\",\n",
    "               \"canine\",\n",
    "               l3,\n",
    "               \"1.1 o 2.1\",\n",
    "               \"1.5 o 2.5\"]\n",
    "\n",
    "calculusElements=[\"calculus\"]\n",
    "petrousElements=[\"petrous\",\n",
    "                 \"petroso\",\n",
    "                 \"petrozo\"]\n",
    "\n",
    "undefinedElements=[\"??\"]\n",
    "otherElements=[\"rib\",\n",
    "               \"fibula\",\n",
    "               \"matoideo\",\n",
    "               \"metatarsal\",\n",
    "               \"humer\",\n",
    "               \"humerus\",\n",
    "               \"lumbar\",\n",
    "               \"craneo\",\n",
    "               \"vertebra\",\n",
    "               \"tibia\",\n",
    "               \"radio\",\n",
    "               \"phalanx\",\n",
    "               \"phalange\",\n",
    "               \"longbone\",\n",
    "               \"femur\",\n",
    "               \"metacarpo\",\n",
    "               \"metatarsus\",\n",
    "               \"tarseano\",\n",
    "               \"hueso\",\n",
    "               \"femur\",\n",
    "              \"tarso\"]\n",
    "\n",
    "noHumanElements=[\"valva\"]    \n",
    "    \n",
    "    \n",
    "rawTab[\"Bone Type\"]=None\n",
    "for index, ele in rawTab[\"Skeletal Element\"].items():\n",
    "    lowerEle=ele.lower()\n",
    "    if bool([ttt for ttt in toothElements if(lowerEle in ttt or str(ttt) in lowerEle)]):\n",
    "        rawTab.at[index,\"Bone Type\"]=\"Tooth\"        \n",
    "    elif bool([ttt for ttt in calculusElements if(lowerEle in ttt or str(ttt) in lowerEle)]):\n",
    "        rawTab.at[index,\"Bone Type\"]=\"Dental Calculus\"\n",
    "    elif bool([ttt for ttt in petrousElements if(lowerEle in ttt or str(ttt) in lowerEle)]):\n",
    "        rawTab.at[index,\"Bone Type\"]=\"Petrous\"\n",
    "    elif bool([ttt for ttt in otherElements if(lowerEle in ttt or str(ttt) in lowerEle)]):\n",
    "        rawTab.at[index,\"Bone Type\"]=\"Other Bone\"\n",
    "    elif bool([ttt for ttt in undefinedElements if(lowerEle in ttt or str(ttt) in lowerEle)]):\n",
    "        rawTab.at[index,\"Bone Type\"]=\"Undefined\"\n",
    "    elif bool([ttt for ttt in noHumanElements if(lowerEle in ttt or str(ttt) in lowerEle)]):\n",
    "        rawTab.at[index,\"Bone Type\"]=\"non Human\"\n",
    "    else:\n",
    "        print(ele+\" issue...\")\n",
    "        \n",
    "        break\n",
    "    \n",
    "\n",
    "###check it is fine...\n",
    "for i in list(set(rawTab[\"Bone Type\"])):\n",
    "    print(i)\n",
    "    print(list(set(rawTab.loc[rawTab[\"Bone Type\"]==i][\"Skeletal Element\"])))\n",
    "\n",
    "if any(rawTab[\"Bone Type\"].isnull()):\n",
    "    print(\"HHAAAAAAA\")\n",
    "    print(rawTab.loc[rawTab[\"Bone Type\"].isnull()][\"Skeletal Element\"])\n",
    "    print(rawTab.loc[rawTab[\"Bone Type\"].isnull()])\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b436daa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###back up\n",
    "table=rawTab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44ce16",
   "metadata": {},
   "source": [
    "#### Prepare json for uploading and updating Sites \n",
    "\n",
    "Get features for Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a861fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Site\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "FeateLabSites = {}\n",
    "for feat in ['Name','Description','Note','Amount','Unit']:\n",
    "    FeateLabSites[feat] = {\"ID\": \"notMeta\"}\n",
    "for feat in data.get(\"data\"):\n",
    "    FeateLabSites[format(feat.get(\"key\"))] = { \"ID\":format(feat.get(\"sampleTypeMetaID\")),\n",
    "                                              \"TYPE\":format(feat.get(\"sampleDataType\"))}\n",
    "#print(FeateLabSites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f4976e",
   "metadata": {},
   "source": [
    "And check that they have been declared SiteDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63064afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in FeateLabSites.keys():\n",
    "    if feat not in SiteDict.keys():\n",
    "        print(feat + \"--> NOT IN DICTIONARY\")\n",
    "        \n",
    "for feat in SiteDict.keys():\n",
    "    if feat not in FeateLabSites.keys():\n",
    "        print(feat + \"--> NOT IN eLAB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf594f7",
   "metadata": {},
   "source": [
    "Now, we make a table with unique entries for the relevant Columns for Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9d00230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Name   Main geographic region    Country  \\\n",
      "0            Isla Barranquita I  Paraná medio / Santa Fe  Argentina   \n",
      "6            Isla Cementerio R3  Paraná medio / Santa Fe  Argentina   \n",
      "8              Puesto Rolancito  Paraná medio / Santa Fe  Argentina   \n",
      "16                          NaN             Buenos Aires  Argentina   \n",
      "18     Tamberías de Bella Vista                 San Juan  Argentina   \n",
      "..                          ...                      ...        ...   \n",
      "836                Finca Flores         Southern Mendoza  Argentina   \n",
      "857  Camping Familiar Cristiano         Southern Mendoza  Argentina   \n",
      "861                   La Cabeza         Southern Mendoza  Argentina   \n",
      "863        El Perdido-El Mallín         Southern Mendoza  Argentina   \n",
      "867              Puesto El Alto         Southern Mendoza  Argentina   \n",
      "\n",
      "    Province / Region       Locality   Latitude  Longitude Site type  \n",
      "0            Santa Fe  Arroyo Aguiar -31.255803 -60.456464       NaN  \n",
      "6            Santa Fe  Arroyo Aguiar -31.386744 -60.561308       NaN  \n",
      "8            Santa Fe        Cayastá -31.181608 -60.089914       NaN  \n",
      "16       Buenos Aires            NaN        NaN        NaN       NaN  \n",
      "18           San Juan            NaN -30.421186 -69.245439       NaN  \n",
      "..                ...            ...        ...        ...       ...  \n",
      "836           Mendoza            NaN -34.660000 -68.420000   Aislado  \n",
      "857           Mendoza            NaN -34.750000 -68.410000   Aislado  \n",
      "861           Mendoza            NaN -36.190000 -68.270000   Aislado  \n",
      "863           Mendoza            NaN -34.665000 -69.605000   Aislado  \n",
      "867           Mendoza            NaN -35.280000 -69.520000   Aislado  \n",
      "\n",
      "[175 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "tableSite=pandas.DataFrame()\n",
    "for col in list(SiteDict.keys()):\n",
    "#        if SiteDict[col] == \"None\":\n",
    "        if SiteDict[col] == \"None\" or SiteDict[col].startswith(\"fixed\"):\n",
    "            continue\n",
    "#        elif SiteDict[col].startswith(\"fixed\"):\n",
    "#            tableSite[col]=SiteDict[col].split(\"_\")[1]\n",
    "        else:\n",
    "            tableSite[col]=table[ SiteDict[col]]\n",
    "tableSite=tableSite.drop_duplicates()\n",
    "print(tableSite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf174f",
   "metadata": {},
   "source": [
    "Now, remove entries for which no Site is reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c03e462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RascovanLabID  Individual ID\n",
      "16       AR0015.1            435\n",
      "17       AR0015.2            435\n",
      "135      AR0120.1           5616\n",
      "156      AR0141.1           7766\n",
      "157      AR0142.1  claromecó-S/N\n",
      "159      AR0144.1           Alum\n"
     ]
    }
   ],
   "source": [
    "print(table[ table[SiteDict['Name']].isnull()][[IndDict['Name'],IndDict['Archaeologist ID']]])\n",
    "tableSite=tableSite.drop(tableSite[tableSite['Name'].isnull()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce095fc0",
   "metadata": {},
   "source": [
    "Now, we check if there no Site name duplicates in that table (dropping Latitude and longitude because sometimes we have the exact location for each sample, in that case we will make a rough average location of the site)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd8c7b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(tableSite.drop(['Latitude','Longitude'],axis=1))\n",
    "duplicatedSites=tableSite.drop(['Latitude','Longitude'],axis=1)\n",
    "duplicatedSites=duplicatedSites.drop_duplicates()\n",
    "duplicatedSites=duplicatedSites[duplicatedSites['Name'].duplicated(keep=False)]\n",
    "if len(duplicatedSites.index) > 0 :\n",
    "    print(\"DUPLICATED SITES... GO BACK TO THE TABLE AND FIX THOSE\")\n",
    "    print(duplicatedSites.sort_values('Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e6200",
   "metadata": {},
   "source": [
    "Now, we average Latitude and Longitude for each Site and check no further duplicated Site appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5f44fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for si in list(tableSite['Name']):\n",
    "    if(len(tableSite.loc[tableSite['Name']==si,].index))>1:\n",
    "        for coor in ['Latitude','Longitude']:\n",
    "            tableSite.loc[tableSite['Name']==si,[coor]]=tableSite.loc[tableSite['Name']==si][coor].mean()\n",
    "            \n",
    "tableSite=tableSite.drop_duplicates()\n",
    "duplicatedSites=tableSite[tableSite['Name'].duplicated(keep=False)]\n",
    "if len(duplicatedSites.index) > 0 :\n",
    "    print(\"DUPLICATED SITES... GO BACK TO THE TABLE AND FIX THOSE\")\n",
    "    duplicatedSites.sort_values('Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d28a6f",
   "metadata": {},
   "source": [
    "Now match the number of digits handled by eLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b59c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableSite['Latitude']=tableSite['Latitude'].round(12)\n",
    "tableSite['Longitude']=tableSite['Longitude'].round(12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe249a",
   "metadata": {},
   "source": [
    "We get all the possible values for checkboxes and dropdown features of Sites and check our Site table is fine.\n",
    "When an entry is null in google spreadsheet, we change it to a NA string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0b2966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Site\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "for feat in data.get(\"data\"):\n",
    "    if feat.get(\"sampleDataType\") == \"CHECKBOX\" or feat.get(\"sampleDataType\") == \"COMBO\":\n",
    "        OptionELAB=feat.get(\"optionValues\")\n",
    "        key=feat.get(\"key\")\n",
    "        tableSite.loc[tableSite[key].isnull(),key] = 'NA'\n",
    "        for tabVal in tableSite[key].unique():\n",
    "            if tabVal not in OptionELAB:\n",
    "                print(tabVal + \" not mapped in eLab for \" + key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2be93e",
   "metadata": {},
   "source": [
    "Now, we make the json for each Site and we upload or update in eLab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d98e3915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already loaded\n",
      "{'recordCount': 1, 'currentPage': 0, 'maxRecords': 1000, 'totalRecords': 1, 'data': [{'owner': 'Pierre LUISI', 'archived': False, 'sampleID': 9482459, 'created': '2021-06-03T13:03:42Z', 'userID': 40629, 'creatorID': 40629, 'storageLayerID': 774999, 'position': 0, 'barcode': '005000009482459', 'sampleType': {'sampleTypeID': 39468, 'userID': 40629, 'groupID': 10684, 'name': 'Site', 'backgroundColor': '900', 'foregroundColor': 'FFF'}, 'sampleTypeID': 39468, 'checkedOut': False, 'parentSampleID': 0, 'name': 'Isla Barranquita I', 'description': 'Nothing entered', 'note': 'Updated from API'}]}\n",
      "metadata already loaded\n",
      "{'Pictures': '', 'Main geographic region': 'Paraná medio / Santa Fe', 'Country': 'Argentina', 'Province / Region': 'Santa Fe', 'Locality': 'Arroyo Aguiar', 'Latitude': '-31.255802777778', 'Longitude': '-60.456463888889', 'Site type': 'nan'}\n"
     ]
    }
   ],
   "source": [
    "###iterate over Sites\n",
    "for index,name in tableSite['Name'].items():     \n",
    "    Data={}\n",
    "    for fea in FeateLabSites.keys():\n",
    "        if FeateLabSites[fea]['ID'] == \"notMeta\":\n",
    "            ###fixed value (from dico)\n",
    "            if SiteDict[fea].startswith(\"fixed\"):\n",
    "                    element=SiteDict[fea].split(\"_\")[1]\n",
    "            elif SiteDict[fea]==\"None\":\n",
    "                    element=\"Nothing entered\"\n",
    "            else:\n",
    "                element=tableSite[fea][index]\n",
    "            Data[fea]=element\n",
    "    ###case of updating\n",
    "    if name in registered['Site'].keys():\n",
    "        #print(name + \" updating\")\n",
    "        patch=True\n",
    "        id=registered['Site'][name]     \n",
    "\n",
    "        Data[\"Note\"]=\"Updated from API\"\n",
    "        DR=requests.patch(url + \"samples/\"+id, headers = headers2,data = Data)\n",
    "    else:\n",
    "        ###case of uploading\n",
    "        #print(name + \" uploading\")\n",
    "        patch=False\n",
    "        Data[\"Note\"]=\"Uploaded from API\"\n",
    "        Data[\"sampleTypeID\"]=types[\"Site\"]\n",
    "        Data[\"Name\"]=name\n",
    "        DR=requests.post(url + \"samples/\", headers = headers2,data = Data)             \n",
    "    ####check the Data loading was correct\n",
    "    if DR.status_code not in [200,204]:\n",
    "        print(\"error for \" + name)\n",
    "        print(DR.status_code)\n",
    "        print(DR.raise_for_status())\n",
    "    ###actualize the registered[\"Site\"] list (checking we did not duplicated anything here)\n",
    "    r=requests.get(url + \"samples/forNames?names=\"+name.replace(\" \",\"%20\"), headers = headers2)\n",
    "    data=r.json()\n",
    "    \n",
    "\n",
    "    sam=data.get(\"data\")\n",
    "    if len(sam)!=1:\n",
    "        print(\"different Site entries (\" + str(len(sam)) + \") for name \"+name)\n",
    "        break\n",
    "    else:\n",
    "        sam=sam[0]\n",
    "        id=str(sam.get(\"sampleID\"))\n",
    "        #print(\"Data OK for \"+ name + \" (\" + id + \")\")\n",
    "        registered[\"Site\"][name]=id\n",
    "\n",
    "    print(\"data already loaded\")\n",
    "    print(data)\n",
    "    ###patch the metaData\n",
    "    ###get loaded values\n",
    "    if patch:\n",
    "        #print(\"patching meta so need to heck if differences\")\n",
    "        MDR=requests.get(url + \"samples/\"+id+\"/meta\", headers = headers2)\n",
    "        if MDR.status_code!=200:\n",
    "            print(\"error querrying meta for \" + name)\n",
    "            break\n",
    "        data=MDR.json().get(\"data\")\n",
    "        metaLoaded={}\n",
    "        for i in data:\n",
    "            metaLoaded[i[\"key\"]]=str(i[\"value\"])\n",
    "    print(\"metadata already loaded\")\n",
    "    print(metaLoaded)\n",
    "    for fea in FeateLabSites.keys():\n",
    "        needToPatch=False\n",
    "        ###get new element to be loaded\n",
    "        if FeateLabSites[fea]['ID'] != \"notMeta\" and FeateLabSites[fea]['TYPE'] != \"FILE\":\n",
    "            ###fixed value (from dico)\n",
    "            if SiteDict[fea].startswith(\"fixed\"):\n",
    "                element=SiteDict[fea].split(\"_\")[1]\n",
    "            elif SiteDict[fea]==\"None\":\n",
    "                element=\"Nothing entered\"\n",
    "            else:\n",
    "                element=tableSite[fea][index]\n",
    "            \n",
    "            ###check if this is a new entry or not\n",
    "            if patch:\n",
    "                ###check if new element is similar to what already loaded\n",
    "                if metaLoaded[fea] != str(element):\n",
    "                    print(\"difference for \" + name + \"(feature: \" + fea + \") \" + format(element) + \" vs loaded : \" + format(metaLoaded[fea]))\n",
    "                    prompt=\"?\"\n",
    "                    #prompt=\"y\"\n",
    "                    while prompt not in [\"y\",\"n\"]:\n",
    "                        prompt = input(\"replace y/n??\")\n",
    "                    if prompt == \"y\":\n",
    "                        needToPatch=True\n",
    "            else:\n",
    "                needToPatch=True\n",
    "\n",
    "            ###if difference ==> we load\n",
    "            if needToPatch:\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabSites[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabSites[fea]['TYPE']}\n",
    "                print(MetaData)\n",
    "                MDR=requests.put(url + \"samples/\"+id+\"/meta\", headers = headers2,data = MetaData)\n",
    "                ####check the MetaData loading was correct\n",
    "                if MDR.status_code not in [200,204]:\n",
    "                    print(\"error for \" + name + \" for feature \" + fea)\n",
    "                    print(MDR.status_code)\n",
    "                    print(MDR.raise_for_status())\n",
    "                    break\n",
    "    #print(\"metadata OK for \"+ name + \" (\" + id + \")\")\n",
    "    \n",
    "    Quant={}\n",
    "    for fea in ['Amount','Unit']:\n",
    "        if SiteDict[fea].startswith(\"fixed\"):\n",
    "            element=SiteDict[fea].split(\"_\")[1]\n",
    "        elif SiteDict[fea]==\"None\":\n",
    "            element=\"Nothing entered\"\n",
    "        else:\n",
    "            element=tableSite[fea][index]\n",
    "        Quant[fea]=element\n",
    "    Quant[\"displayUnit\"]=Quant[\"Unit\"].capitalize()\n",
    "    Quant[\"fullAmount\"]=Quant[\"Amount\"]\n",
    "    QR=requests.put(url + \"samples/\" + id + \"/quantity\", headers = headers2, data = Quant)\n",
    "    if QR.status_code not in [200,204]:\n",
    "        print(\"error for \" + name + \" for quantity\")\n",
    "        print(QR.status_code)\n",
    "        print(QR.raise_for_status())\n",
    "    #else:\n",
    "        #print(\"quantity OK for \"+ name + \" (\" + id + \")\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dc4374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'owner': 'Pierre LUISI', 'archived': False, 'sampleID': 9507924, 'created': '2021-07-21T12:11:14Z', 'userID': 40629, 'creatorID': 40629, 'storageLayerID': 774998, 'position': 0, 'barcode': '005000009507924', 'sampleType': {'sampleTypeID': 39466, 'userID': 40629, 'groupID': 10684, 'name': 'Individual', 'backgroundColor': 'F00', 'foregroundColor': 'FFF'}, 'sampleTypeID': 39466, 'checkedOut': False, 'parentSampleID': 9482459, 'name': 'AR0001', 'description': 'Nothing entered', 'note': 'Updated from API'}\n"
     ]
    }
   ],
   "source": [
    "def BadRequest(myReq,code=200):\n",
    "    return(myReq.status_code !=code)\n",
    "\n",
    "\n",
    "id=registered[\"Individual\"][\"AR0001\"]\n",
    "getReq=requests.get(url + \"samples/\"+id, headers = headers2)\n",
    "if BadRequest(getReq,200):\n",
    "        print(\"error\")\n",
    "        print(getReq.status_code)\n",
    "        print(getReq.raise_for_status())\n",
    "\n",
    "\n",
    "print(getReq.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23fc43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Data={'description': getReq.json()[\"description\"]+\"\\n TEST HOHEHEINBON\"}\n",
    "patchReq=requests.patch(url + \"samples/\"+id, headers = headers2,data=Data)\n",
    "if BadRequest(patchReq,204):\n",
    "        print(\"error\")\n",
    "        print(patchReq.status_code)\n",
    "        print(patchReq.raise_for_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3496c7d",
   "metadata": {},
   "source": [
    "#### Prepare json for uploading and updating Individuals\n",
    "Get features for Individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f9a6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Individual\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "FeateLabInds = {}\n",
    "for feat in ['Name','Description','Note','Amount','Unit',\"parentSampleID\"]:\n",
    "    FeateLabInds[feat] = {\"ID\": \"notMeta\"}\n",
    "for feat in data.get(\"data\"):\n",
    "    FeateLabInds[format(feat.get(\"key\"))] = { \"ID\":format(feat.get(\"sampleTypeMetaID\")),\n",
    "                                              \"TYPE\":format(feat.get(\"sampleDataType\"))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38eb545",
   "metadata": {},
   "source": [
    "And check that they have been declared IndDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "085f4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in FeateLabInds.keys():\n",
    "    if feat not in IndDict.keys():\n",
    "        print(feat + \"--> NOT IN DICTIONARY\")\n",
    "        \n",
    "for feat in IndDict.keys():\n",
    "    if feat not in FeateLabInds.keys():\n",
    "        print(feat + \"--> NOT IN eLAB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4457df1",
   "metadata": {},
   "source": [
    "Now we make a table of unique entries for relevant columns for individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2366630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Name      parentSampleID            Archaeologist ID  \\\n",
      "0    AR0001  Isla Barranquita I                         252   \n",
      "1    AR0002  Isla Barranquita I                         253   \n",
      "2    AR0003  Isla Barranquita I                         255   \n",
      "3    AR0004  Isla Barranquita I                         256   \n",
      "4    AR0005  Isla Barranquita I                         154   \n",
      "..      ...                 ...                         ...   \n",
      "866  AR0692    Villa 25 de Mayo                       V25-1   \n",
      "867  AR0693      Puesto El Alto              Puesto El Alto   \n",
      "868  AR0694         Ojo de Agua                       OA1-9   \n",
      "870  AR0695         Ojo de Agua                     OA1-4-3   \n",
      "871  AR0696          Capiz Alto  Capiz Alto 1 (prospección)   \n",
      "\n",
      "                       Archaeologist group           Site Name  \\\n",
      "0                         Mariano del Papa  Isla Barranquita I   \n",
      "1                         Mariano del Papa  Isla Barranquita I   \n",
      "2                         Mariano del Papa  Isla Barranquita I   \n",
      "3                         Mariano del Papa  Isla Barranquita I   \n",
      "4                         Mariano del Papa  Isla Barranquita I   \n",
      "..                                     ...                 ...   \n",
      "866  Gustavo Neme / Fito Gil / Eva Peralta    Villa 25 de Mayo   \n",
      "867  Gustavo Neme / Fito Gil / Eva Peralta      Puesto El Alto   \n",
      "868  Gustavo Neme / Fito Gil / Eva Peralta         Ojo de Agua   \n",
      "870  Gustavo Neme / Fito Gil / Eva Peralta         Ojo de Agua   \n",
      "871  Gustavo Neme / Fito Gil / Eva Peralta          Capiz Alto   \n",
      "\n",
      "                                 Date Datation method                   Age  \\\n",
      "0                                1200             C14                   NaN   \n",
      "1                                1200             C14                   NaN   \n",
      "2                                1200             C14                   NaN   \n",
      "3                                1200             C14                   NaN   \n",
      "4                                1200             C14                   NaN   \n",
      "..                                ...             ...                   ...   \n",
      "866         - (from site spreadsheet)             NaN     >20 años (adulto)   \n",
      "867       2870 (from ind spreadsheet)             NaN     >20 años (adulto)   \n",
      "868  1943/1761 (from ind spreadsheet)             NaN      1-4.9 (infantil)   \n",
      "870  1943/1761 (from ind spreadsheet)             NaN  0-0.9 años (neonato)   \n",
      "871    450/385 (from ind spreadsheet)             NaN     >20 años (adulto)   \n",
      "\n",
      "    Gender  \n",
      "0      NaN  \n",
      "1      NaN  \n",
      "2      NaN  \n",
      "3      NaN  \n",
      "4      NaN  \n",
      "..     ...  \n",
      "866     IN  \n",
      "867     IN  \n",
      "868     IN  \n",
      "870     IN  \n",
      "871     IN  \n",
      "\n",
      "[666 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "tableInd=pandas.DataFrame()\n",
    "for col in list(IndDict.keys()):\n",
    "#        if SiteDict[col] == \"None\":\n",
    "        if IndDict[col] == \"None\" or IndDict[col].startswith(\"fixed\"):\n",
    "            continue\n",
    "        else:\n",
    "            tableInd[col]=table[ IndDict[col]]\n",
    "#print(tableInd)\n",
    "duplicatedInd=tableInd[tableInd['Name'].duplicated(keep=False)]\n",
    "if len(duplicatedInd.index) > 0 :\n",
    "    print(\"DUPLICATED Inds... GO BACK TO THE TABLE AND FIX THOSE\")\n",
    "    print(duplicatedInd.sort_values('Name'))\n",
    "    \n",
    "tableInd['Name']=tableInd['Name'].str.split(\".\",expand=True)[0]\n",
    "tableInd=tableInd.drop_duplicates()\n",
    "\n",
    "print(tableInd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688558d",
   "metadata": {},
   "source": [
    "Change gender To Male / Female / NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c8b6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Male={\"M\",\n",
    "     \"male\",\n",
    "     \"Male\",\n",
    "     \"Male (Det.)\",\n",
    "     \"Masculino\",\n",
    "     \"masculino\",\n",
    "     \"Male (Estimated)\",\n",
    "     \"Male (Est.)\"}\n",
    "Female={\"F\",\n",
    "        \"female\",\n",
    "        \"Female\",\n",
    "        \"Female (Det.)\",\n",
    "        \"femenino\",\n",
    "        \"Femenino\",\n",
    "        \"Female (Estimated)\",\n",
    "        \"Female (Est.)\"}\n",
    "NA={\"I\",\"F?\",\"M?\",\"Unknown\",\"ND\",\"IN\",\"-\",\"Female?\",\"NA\",\"Undefined\",\"indet\",\"Indeterminado\",\"?\",\"No determinado\"}\n",
    "\n",
    "tableInd.loc[tableInd[\"Gender\"].isnull(),\"Gender\"]='NA'\n",
    "for index, ele in tableInd[\"Gender\"].items():\n",
    "    if ele in Male:\n",
    "        tableInd.at[index,\"Gender\"]=\"Male\"\n",
    "    elif ele in Female:\n",
    "        tableInd.at[index,\"Gender\"]=\"Female\"\n",
    "    elif ele in NA:\n",
    "        tableInd.at[index,\"Gender\"]=\"NA\"\n",
    "    else:\n",
    "        print(ele + \" not defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249cef70",
   "metadata": {},
   "source": [
    "We get all the possible values for checkboxes and dropdown features of Individuals and check our Individual table is fine. When an entry is null in google spreadsheet, we change it to a NA string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4a81bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Individual\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "for feat in data.get(\"data\"):\n",
    "    if feat.get(\"sampleDataType\") == \"CHECKBOX\" or feat.get(\"sampleDataType\") == \"COMBO\":\n",
    "        OptionELAB=feat.get(\"optionValues\")\n",
    "        key=feat.get(\"key\")\n",
    "        tableInd.loc[tableInd[key].isnull(),key]='NA'\n",
    "        for tabVal in tableInd[key].unique():\n",
    "            if tabVal not in OptionELAB:\n",
    "                print(\"--\" + tabVal + \"-- not mapped in eLab for \" + key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263eb3f",
   "metadata": {},
   "source": [
    "Check if duplicated entries (meaning that some fields are inconsistent across different lines for same individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b0792dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicatedInd=tableInd.drop_duplicates()\n",
    "duplicatedInd=duplicatedInd[duplicatedInd['Name'].duplicated(keep=False)]\n",
    "if len(duplicatedInd.index) > 0 :\n",
    "    print(\"DUPLICATED Individuals... GO BACK TO THE TABLE AND FIX THOSE\")\n",
    "    print(duplicatedInd.sort_values('Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9203a95",
   "metadata": {},
   "source": [
    "Now, we make the json for each Individual and we upload or update in eLab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1507086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference for AR0001(feature: Date) 1200 vs loaded : 1200 BP\n",
      "replace y/n??n\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "###iterate over Individuals\n",
    "for index,name in tableInd['Name'].items():\n",
    "    ####load the Data!\n",
    "    Data={}\n",
    "    for fea in FeateLabInds.keys():\n",
    "        if FeateLabInds[fea]['ID'] == \"notMeta\":\n",
    "            ###fixed value (from dico)\n",
    "            if IndDict[fea].startswith(\"fixed\"):\n",
    "                element=IndDict[fea].split(\"_\")[1]\n",
    "            elif IndDict[fea]==\"None\":\n",
    "                element=\"Nothing entered\"\n",
    "            elif fea == \"parentSampleID\":\n",
    "                #element=tableInd[fea][index]+\"|\"+registered[IndDict[fea]][tableInd[fea][index]]\n",
    "                if format(tableInd[fea][index])==\"nan\":\n",
    "                    element=0\n",
    "                else:\n",
    "                    element=registered[IndDict[fea]][tableInd[fea][index]]\n",
    "            else:\n",
    "                element=tableInd[fea][index]\n",
    "            Data[fea]=element\n",
    "    ###case of updating\n",
    "    if name in registered['Individual'].keys():\n",
    "        #print(name + \"updating\")\n",
    "        patch=True\n",
    "        id=registered['Individual'][name]\n",
    "        Data[\"Note\"]=\"Updated from API\"\n",
    "        ###QUERY CAMPO\n",
    "        ##SI CAMPO ELAB <> CAMPO TABLA:\n",
    "        ##        proimpt: update???\n",
    "            \n",
    "        DR=requests.patch(url + \"samples/\"+id, headers = headers2,data = Data)\n",
    "    else:\n",
    "        ###case of uploading\n",
    "        #print(name + \"uploading\")\n",
    "        patch=False\n",
    "        Data[\"Note\"]=\"Uploaded from API\"\n",
    "        Data[\"sampleTypeID\"]=types[\"Individual\"]\n",
    "        Data[\"Name\"]=name\n",
    "        DR=requests.post(url + \"samples/\", headers = headers2,data = Data)             \n",
    "    ####check the Data loading was correct\n",
    "    if DR.status_code not in [200,204]:\n",
    "        print(\"error for \" + name)\n",
    "        print(DR.status_code)\n",
    "        print(DR.raise_for_status())\n",
    "    ###actualize the registered[\"Site\"] list (checking we did not duplicated anything here)\n",
    "    r=requests.get(url + \"samples/forNames?names=\"+name, headers = headers2)\n",
    "    data=r.json()\n",
    "    sam=data.get(\"data\")\n",
    "    if len(sam)!=1:\n",
    "        print(\"different Individual entries (\" + str(len(sam)) + \") for name \"+name)\n",
    "        break\n",
    "    else:\n",
    "        sam=sam[0]\n",
    "        id=str(sam.get(\"sampleID\"))\n",
    "        #print(\"Data OK for \"+ name + \" (\" + id + \")\")\n",
    "        registered[\"Individual\"][name]=id\n",
    "\n",
    "    ###patch the metaData\n",
    "    if patch:\n",
    "        #print(\"patching meta so need to heck if differences for \"+name)\n",
    "        MDR=requests.get(url + \"samples/\"+id+\"/meta\", headers = headers2)\n",
    "        if MDR.status_code!=200:\n",
    "            print(\"error querrying meta for \" + name)\n",
    "            break\n",
    "        data=MDR.json().get(\"data\")\n",
    "        metaLoaded={}\n",
    "        for i in data:\n",
    "            metaLoaded[i[\"key\"]]=str(i[\"value\"])\n",
    "\n",
    "    for fea in FeateLabInds.keys():\n",
    "        needToPatch=False\n",
    "        ###get new element to be loaded\n",
    "        if FeateLabInds[fea]['ID'] != \"notMeta\" and FeateLabInds[fea]['TYPE'] != \"FILE\":\n",
    "            ###fixed value (from dico)\n",
    "            if IndDict[fea].startswith(\"fixed\"):\n",
    "                element=IndDict[fea].split(\"_\")[1]\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabInds[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabInds[fea]['TYPE']}\n",
    "            elif IndDict[fea]==\"None\":\n",
    "                element=\"Nothing entered\"\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabInds[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabInds[fea]['TYPE']}\n",
    "            elif FeateLabInds[fea]['TYPE'] == \"SAMPLELINK\" and format(tableInd[fea][index])!=\"nan\":\n",
    "                samples=[]\n",
    "                splitted=tableInd[fea][index].split(\",\")\n",
    "                splitted=list(dict.fromkeys(splitted))\n",
    "                for sisi in splitted:\n",
    "                    IDsisi=registered[IndDict[fea]][sisi]\n",
    "                    samples.append({\"sampleID\": IDsisi,\"name\": sisi})\n",
    "                    if sisi != splitted[0]:\n",
    "                        element=element+\",\"+sisi+\"|\"+IDsisi\n",
    "                    else:\n",
    "                        element=sisi+\"|\"+IDsisi\n",
    "                MetaData={\n",
    "                    \"sampleTypeMetaID\": int(FeateLabInds[fea]['ID']),\n",
    "                    \"sampleDataType\": FeateLabInds[fea]['TYPE'],\n",
    "                    \"samples\": samples,\n",
    "                    \"key\": fea,\n",
    "                    \"value\": element\n",
    "                }\n",
    "            else:\n",
    "                element=tableInd[fea][index]\n",
    "                if format(element)==\"nan\" :\n",
    "                    element=\"NA\"\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabInds[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabInds[fea]['TYPE']}\n",
    "            \n",
    "            ###check if this is a new entry or not\n",
    "            if patch:\n",
    "                ###check if new element is similar to what already loaded\n",
    "                if metaLoaded[fea] != str(element):\n",
    "                    print(\"difference for \" + name + \"(feature: \" + fea + \") \" + element + \" vs loaded : \" + metaLoaded[fea])\n",
    "                    #prompt=\"y\"\n",
    "                    prompt=\"?\"\n",
    "                    while prompt not in [\"y\",\"n\"]:\n",
    "                        prompt = input(\"replace y/n??\")\n",
    "                    if prompt == \"y\":\n",
    "                        needToPatch=True\n",
    "            else:\n",
    "                needToPatch=True\n",
    "    \n",
    "            if needToPatch:\n",
    "                #print(MetaData)      \n",
    "                MDR=requests.put(url + \"samples/\"+id+\"/meta\", headers = headers2,data = MetaData)\n",
    "                ####check the MetaData loading was correct\n",
    "                if MDR.status_code not in [200,204]:\n",
    "                    print(\"error for \" + name + \" for feature \" + fea)\n",
    "                    print(MDR.status_code)\n",
    "                    print(MDR.raise_for_status())\n",
    "                    break\n",
    "    #print(\"metadata OK for \"+ name + \" (\" + id + \")\")\n",
    "    ###patch the quantity\n",
    "    Quant={}\n",
    "    for fea in ['Amount','Unit']:\n",
    "        if IndDict[fea].startswith(\"fixed\"):\n",
    "            element=IndDict[fea].split(\"_\")[1]\n",
    "        elif IndDict[fea]==\"None\":\n",
    "            element=\"Nothing entered\"\n",
    "        else:\n",
    "            element=tableInd[fea][index]\n",
    "        Quant[fea]=element\n",
    "    Quant[\"displayUnit\"]=Quant[\"Unit\"].capitalize()\n",
    "    Quant[\"fullAmount\"]=Quant[\"Amount\"]\n",
    "    QR=requests.put(url + \"samples/\" + id + \"/quantity\", headers = headers2, data = Quant)\n",
    "    if QR.status_code not in [200,204]:\n",
    "        print(\"error for \" + name + \" for quantity\")\n",
    "        print(QR.status_code)\n",
    "        print(QR.raise_for_status())\n",
    "    #else:\n",
    "    #    print(\"quantity OK for \"+ name + \" (\" + id + \")\")\n",
    "\n",
    "print(\"finished\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56913a76",
   "metadata": {},
   "source": [
    "#### Prepare json for uploading and updating Skeleton elements\n",
    "Get features for Skeleton elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8589b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Skeleton Element\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "FeateLabSkel = {}\n",
    "for feat in ['Name','Description','Note','Amount','Unit',\"parentSampleID\"]:\n",
    "    FeateLabSkel[feat] = {\"ID\": \"notMeta\"}\n",
    "for feat in data.get(\"data\"):\n",
    "    FeateLabSkel[format(feat.get(\"key\"))] = { \"ID\":format(feat.get(\"sampleTypeMetaID\")),\n",
    "                                              \"TYPE\":format(feat.get(\"sampleDataType\"))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af2be2",
   "metadata": {},
   "source": [
    "And check that they have been declared SkeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49e315f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in FeateLabSkel.keys():\n",
    "    if feat not in SkeDict.keys():\n",
    "        print(feat + \"--> NOT IN DICTIONARY\")\n",
    "        \n",
    "for feat in SkeDict.keys():\n",
    "    if feat not in FeateLabSkel.keys():\n",
    "        print(feat + \"--> NOT IN eLAB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2f99c",
   "metadata": {},
   "source": [
    "We get all the possible values for checkboxes and dropdown features of Skeleton Elements and check our Skeleton Elements table is fine. When an entry is null in google spreadsheet, we change it to a NA string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e309e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Skeleton Element\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "for feat in data.get(\"data\"):\n",
    "    if feat.get(\"sampleDataType\") == \"CHECKBOX\" or feat.get(\"sampleDataType\") == \"COMBO\":\n",
    "        OptionELAB=feat.get(\"optionValues\")\n",
    "        key=feat.get(\"key\")\n",
    "        #table.loc[tableInd[key].isnull(),key]='NA'\n",
    "        for tabVal in table[SkeDict[key]].unique():\n",
    "            if tabVal not in OptionELAB:\n",
    "                print(\"--\" + tabVal + \"-- not mapped in eLab for \" + key)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302d951",
   "metadata": {},
   "source": [
    "Download extract file from metapaleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41449548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import paramiko \n",
    "\n",
    "user = open(\"credentials/sftpUser\",\"r\").readline().strip().split(\"\\t\")\n",
    "psw=user[1]\n",
    "user=user[0]\n",
    "\n",
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "ssh.connect('sftpcampus.pasteur.fr', username=user, password=psw)\n",
    "sftp = ssh.open_sftp()\n",
    "localpath = './DDBB_extracts.csv'\n",
    "remotepath = '/pasteur/entites/metapaleo/Research/ERC-project/Samples/LabellingExtracts/DDBB_extracts.csv'\n",
    "sftp.get(remotepath,localpath)\n",
    "sftp.close()\n",
    "ssh.close()\n",
    "\n",
    "extractTable=pandas.read_csv(\"DDBB_extracts.csv\",delimiter=\";\")\n",
    "extractTable[\"RascovanLabID\"]=None\n",
    "for index,name in extractTable[\"ExtractID\"].items():\n",
    "    extractTable.loc[index,\"RascovanLabID\"]=\".\".join(name.split(\".\")[0:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9ef17",
   "metadata": {},
   "source": [
    "and compile it to the table for \"Pictures\" and \"General Sample Comment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8bfc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "skelTablefromEx=pandas.DataFrame()\n",
    "for key in [\"RascovanLabID\",\"Pictures\",\"GeneralSampleComment\"]:\n",
    "    skelTablefromEx[key]=extractTable[key]\n",
    "\n",
    "    \n",
    "skelTablefromEx=skelTablefromEx.drop_duplicates()\n",
    "###duplicated (UNEXPECTED!)\n",
    "dupSkefromEx=skelTablefromEx.loc[skelTablefromEx[\"RascovanLabID\"].duplicated(keep=False)]\n",
    "if len(dupSkefromEx):\n",
    "    print(\"DUPLICATED rascoIDs in extract file\")\n",
    "    print(dupSkefromEx)\n",
    "\n",
    "    \n",
    "table[\"DrillingPictures\"]=None\n",
    "table[\"GeneralSampleComment\"]=None\n",
    "\n",
    "for index,rascoID in table[\"RascovanLabID\"].items():\n",
    "    #print(str(index)+\" \"+rascoID)\n",
    "    pic=\"NA\"\n",
    "    com=\"NA\"\n",
    "    if rascoID in list(skelTablefromEx[\"RascovanLabID\"]):\n",
    "        if list(skelTablefromEx.loc[skelTablefromEx[\"RascovanLabID\"]==rascoID,\"Pictures\"])[0] == \"T\":\n",
    "            pic=\"/pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/\"+rascoID\n",
    "        com=list(skelTablefromEx.loc[skelTablefromEx[\"RascovanLabID\"]==rascoID,\"GeneralSampleComment\"])[0]\n",
    "    #print(rascoID+\" \"+com+\" \"+pic)\n",
    "    table.loc[index,\"DrillingPictures\"]=pic\n",
    "    table.loc[index,\"GeneralSampleComment\"]=com\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c0fab",
   "metadata": {},
   "source": [
    "Check if duplicated entries (meaning that some fields are inconsistent across different lines for same skeleton element)\n",
    "Check if no duplicated skeleton element nor archaeologist ID for skel element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74436081",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicatedAR=table[table[SkeDict['Name']].duplicated(keep=False)]\n",
    "if len(duplicatedAR.index):\n",
    "    print(\"DUPLICATED rascoIDs\")\n",
    "    print(duplicatedAR)\n",
    "\n",
    "duplicatedSam=table.loc[table[SkeDict['Archaeologist sample ID']].duplicated(keep=False)]\n",
    "if len(duplicatedSam.index):\n",
    "    print(\"DUPLICATED archeologist ID\")\n",
    "    print(duplicatedSam[[SkeDict[x] for x in [\"Name\",\"Archaeologist sample ID\"]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f81584",
   "metadata": {},
   "source": [
    "Now, we make the json for each Skeleton element and we upload or update in eLab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b077b5a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PicturePath</th>\n",
       "      <th>1st Batch</th>\n",
       "      <th>Second Teeth batch</th>\n",
       "      <th>Expediente</th>\n",
       "      <th>Geographic Zone</th>\n",
       "      <th>Archaeologists Group</th>\n",
       "      <th>Sample ID</th>\n",
       "      <th>Individual ID</th>\n",
       "      <th>RascovanLabID</th>\n",
       "      <th>Skeletal Element</th>\n",
       "      <th>...</th>\n",
       "      <th>Pathologies (observed or suspected)</th>\n",
       "      <th>Isotopes done</th>\n",
       "      <th>Isotopes needed</th>\n",
       "      <th>Observations</th>\n",
       "      <th>Observation Pierre / Maria</th>\n",
       "      <th>LatChanged</th>\n",
       "      <th>LongChanged</th>\n",
       "      <th>Bone Type</th>\n",
       "      <th>DrillingPictures</th>\n",
       "      <th>GeneralSampleComment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>/pasteur/entites/metapaleo/Research/ERC-projec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tucuman_3aTanda</td>\n",
       "      <td>Piedemonte central de Tucumán</td>\n",
       "      <td>Gabriel Eduardo Miguez</td>\n",
       "      <td>Sitio Anta YACU 2-I1-1</td>\n",
       "      <td>Sitio Anta YACU 2-I1</td>\n",
       "      <td>AR0036.1</td>\n",
       "      <td>Tooth fragment</td>\n",
       "      <td>...</td>\n",
       "      <td>Sin identificar debido al mal estado de preser...</td>\n",
       "      <td>NO</td>\n",
       "      <td>Es de ALTA IMPORTANCIA (***) realizar análisis...</td>\n",
       "      <td>Individuo identificado en vasija-urna nº 2 de ...</td>\n",
       "      <td>sent 7 fragments. received very very fragmente...</td>\n",
       "      <td>-26.777472</td>\n",
       "      <td>-65.319472</td>\n",
       "      <td>Tooth</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          PicturePath 1st Batch  \\\n",
       "47  /pasteur/entites/metapaleo/Research/ERC-projec...       NaN   \n",
       "\n",
       "   Second Teeth batch       Expediente                Geographic Zone  \\\n",
       "47                NaN  Tucuman_3aTanda  Piedemonte central de Tucumán   \n",
       "\n",
       "      Archaeologists Group               Sample ID         Individual ID  \\\n",
       "47  Gabriel Eduardo Miguez  Sitio Anta YACU 2-I1-1  Sitio Anta YACU 2-I1   \n",
       "\n",
       "   RascovanLabID Skeletal Element  ...  \\\n",
       "47      AR0036.1   Tooth fragment  ...   \n",
       "\n",
       "                  Pathologies (observed or suspected) Isotopes done  \\\n",
       "47  Sin identificar debido al mal estado de preser...            NO   \n",
       "\n",
       "                                      Isotopes needed  \\\n",
       "47  Es de ALTA IMPORTANCIA (***) realizar análisis...   \n",
       "\n",
       "                                         Observations  \\\n",
       "47  Individuo identificado en vasija-urna nº 2 de ...   \n",
       "\n",
       "                           Observation Pierre / Maria LatChanged LongChanged  \\\n",
       "47  sent 7 fragments. received very very fragmente... -26.777472  -65.319472   \n",
       "\n",
       "   Bone Type DrillingPictures GeneralSampleComment  \n",
       "47     Tooth               NA                   NA  \n",
       "\n",
       "[1 rows x 39 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.loc[table[SkeDict['Name']]==\"AR0036.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2484732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference for AR0044.1(feature: Observation Drilling) no calculus, bad preservation with brown colour of the root and full of cracks and dirt vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0047.1(feature: Observation Drilling) no calculus observed, small incisive with longitudinal cracks and brown colour, bad preservation vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0048.1(feature: Observation Drilling) may have calculus, full of black consolidations on the root, badly preserved appears soft and chipped, root apex broke with slight pressure during decontamination vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0048.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0048.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0051.1(feature: Observation Drilling) some calculus, longitudinal cracks on the crown, yellow colour, not great preservation vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0051.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0051.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0067.1(feature: Observation Drilling) very little calculus, many consolidations, brown spots, longitudinal cracks on crown, difficult incisive to cut vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0067.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0067.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0071.1(feature: Observation Drilling) tooth is full of black dots including on the root apex, longitudinal cracks all over the tooth, bad preservation, difficult incisive to cut vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0071.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0071.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0077.1(feature: Observation Drilling) good quality tooth, white and sturdy vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0077.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0077.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0147.1(feature: Observation Drilling) brown-ish root apex only vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0147.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0147.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0159.1(feature: Observation Drilling) dirt, consolidations, brown colour vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0159.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0159.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0160.1(feature: Observation Drilling) brown-ish tooth, bad preservation vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0160.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0160.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0161.1(feature: Observation Drilling) no calculus, full of black and yellow consolidations on the crown vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0161.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0161.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0169.1(feature: Observation Drilling) caries on the crown, tooth full of dirt, molar with 4 roots only one was ok, a lot of calculus but may be contaminated with dirt, tooth crown broke upon sampling, gave 3 min instead of 2 with UV decontamination vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0169.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0169.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0180.1(feature: Observation Drilling) a lot of calculus, extra bone growth on the root apex and body of the root, many transversal cracks vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0180.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0180.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0181.1(feature: Observation Drilling) very small tooth, broken crown vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0181.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0181.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0182.1(feature: Observation Drilling) nice big molar, seems like gums were pushed a little down on the root so little calculus dots on upper root vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0182.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0182.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0183.1(feature: Observation Drilling) incisive, black growing material at the root apex, I tried to take it off with decontamination or scalpel but it does not come off and seem to be part of the apex, could be pathology or not, may need sampling another portion vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0183.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0183.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0184.1(feature: Observation Drilling) broken crown and very little calculus on the front of the tooth, yellow colour and dirt vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0184.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0184.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0187.1(feature: Observation Drilling) dirt on the root and some cracks on the crown, roots were fused but it was easy to just take one of them vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0187.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0187.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0198.1(feature: Observation Drilling) brown-ish root apex, brown consolidations on crown, many cracks and not well preserved vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0198.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0198.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0256.1(feature: Observation Drilling) caries, crown very consumed by human activity like work or habit, dirt, gums seemed to be pushed lower on the roots due to caries vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0256.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0256.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0278.1(feature: Observation Drilling) full of white consolidations, full of dirt, caries, no calculus vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0278.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0278.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0282.1(feature: Observation Drilling) very big caries that was full of dirt, cleaned with decon vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0282.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0282.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0286.1(feature: Observation Drilling) little caries, a lot of dirt, consumed crown by human activity or habit, yellow calculus vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0286.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0286.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0289.1(feature: Observation Drilling) full of dirt, consolidations vs loaded : NA\n",
      "replace y/n??y\n",
      "difference for AR0289.1(feature: Pictures Drilling) /pasteur/entites/metapaleo/Research/ERC-project/Samples/pictures/Drilling/AR0289.1 vs loaded : NA\n",
      "replace y/n??y\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "###iterate over Skeleton Elements\n",
    "for index,name in table[SkeDict['Name']].items():\n",
    "    ####load the Data!\n",
    "    Data={}\n",
    "    for fea in FeateLabSkel.keys():\n",
    "        if FeateLabSkel[fea]['ID'] == \"notMeta\":\n",
    "            ###fixed value (from dico)\n",
    "            if SkeDict[fea].startswith(\"fixed\"):\n",
    "                element=SkeDict[fea].split(\"_\")[1]\n",
    "            elif SkeDict[fea]==\"None\":\n",
    "                element=\"Nothing entered\"\n",
    "            elif fea == \"parentSampleID\":\n",
    "                element=name.split(\".\")[0]\n",
    "                if element not in registered[\"Individual\"]:\n",
    "                    print(\"can't not set \"+element+\" as parent sample\")\n",
    "                    break\n",
    "                element=registered[\"Individual\"][element]\n",
    "            else:\n",
    "                element=table[SkeDict[fea]][index]\n",
    "            Data[fea]=element\n",
    "    ###case of updating\n",
    "    if name in registered['Skeleton Element'].keys():\n",
    "        #print(name + \"updating\")\n",
    "        patch=True\n",
    "        id=registered['Skeleton Element'][name]\n",
    "        Data[\"Note\"]=\"Updated from API\"\n",
    "        ###QUERY CAMPO\n",
    "        ##SI CAMPO ELAB <> CAMPO TABLA:\n",
    "        ##        proimpt: update???\n",
    "            \n",
    "        DR=requests.patch(url + \"samples/\"+id, headers = headers2,data = Data)\n",
    "    else:\n",
    "        ###case of uploading\n",
    "        #print(name + \"uploading\")\n",
    "        patch=False\n",
    "        Data[\"Note\"]=\"Uploaded from API\"\n",
    "        Data[\"sampleTypeID\"]=types[\"Skeleton Element\"]\n",
    "        Data[\"Name\"]=name\n",
    "        DR=requests.post(url + \"samples/\", headers = headers2,data = Data)             \n",
    "    ####check the Data loading was correct\n",
    "    if DR.status_code not in [200,204]:\n",
    "        print(\"error for \" + name)\n",
    "        print(DR.status_code)\n",
    "        print(DR.raise_for_status())\n",
    "    ###actualize the registered[\"Site\"] list (checking we did not duplicated anything here)\n",
    "    r=requests.get(url + \"samples/forNames?names=\"+name, headers = headers2)\n",
    "    data=r.json()\n",
    "    sam=data.get(\"data\")\n",
    "    if len(sam)!=1:\n",
    "        print(\"different Skeleton Element entries (\" + str(len(sam)) + \") for name \"+name)\n",
    "        break\n",
    "    else:\n",
    "        sam=sam[0]\n",
    "        id=str(sam.get(\"sampleID\"))\n",
    "        #print(\"Data OK for \"+ name + \" (\" + id + \")\")\n",
    "        registered[\"Skeleton Element\"][name]=id\n",
    "\n",
    "    ###patch the metaData\n",
    "    if patch:\n",
    "        #print(\"patching meta so need to heck if differences for \"+name)\n",
    "        MDR=requests.get(url + \"samples/\"+id+\"/meta\", headers = headers2)\n",
    "        if MDR.status_code!=200:\n",
    "            print(\"error querrying meta for \" + name)\n",
    "            break\n",
    "        data=MDR.json().get(\"data\")\n",
    "        metaLoaded={}\n",
    "        for i in data:\n",
    "            metaLoaded[i[\"key\"]]=str(i[\"value\"])\n",
    "\n",
    "    for fea in FeateLabSkel.keys():\n",
    "        needToPatch=False\n",
    "        ###get new element to be loaded\n",
    "        if FeateLabSkel[fea]['ID'] != \"notMeta\" and FeateLabSkel[fea]['TYPE'] != \"FILE\":\n",
    "            ###fixed value (from dico)\n",
    "            if SkeDict[fea].startswith(\"fixed\"):\n",
    "                element=SkeDict[fea].split(\"_\")[1]\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabSkel[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabSkel[fea]['TYPE']}\n",
    "            elif SkeDict[fea]==\"None\":\n",
    "                element=\"Nothing entered\"\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabSkel[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabSkel[fea]['TYPE']}\n",
    "            elif fea == \"From Individual\":\n",
    "                sisi=name.split(\".\")[0]\n",
    "                IDsisi=registered[\"Individual\"][sisi]\n",
    "                element=sisi+\"|\"+IDsisi\n",
    "                MetaData={\n",
    "                   \"sampleTypeMetaID\": int(FeateLabSkel[fea]['ID']),\n",
    "                   \"sampleDataType\": FeateLabSkel[fea]['TYPE'],\n",
    "                   \"samples\": sisi,\n",
    "                    \"key\": fea,\n",
    "                    \"value\": element\n",
    "                }\n",
    "            else:\n",
    "                element=table[SkeDict[fea]][index]\n",
    "                if format(element)==\"nan\" :\n",
    "                    element=\"NA\"\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabSkel[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabSkel[fea]['TYPE']}\n",
    "            \n",
    "            ###check if this is a new entry or not\n",
    "            if patch:\n",
    "                ###check if new element is similar to what already loaded\n",
    "                if fea not in metaLoaded.keys(): \n",
    "                    needToPatch=True\n",
    "                elif metaLoaded[fea] != str(element):\n",
    "                    print(\"difference for \" + name + \"(feature: \" + fea + \") \" + element + \" vs loaded : \" + metaLoaded[fea])\n",
    "                    #prompt=\"y\"\n",
    "                    if fea == \"Pictures Labelling\" or fea == \"Exportation Permit Number\":\n",
    "                        prompt=\"y\"\n",
    "                    else:\n",
    "                        prompt=\"?\"\n",
    "                    while prompt not in [\"y\",\"n\"]:\n",
    "                        prompt = input(\"replace y/n??\")\n",
    "                    if prompt == \"y\":\n",
    "                        needToPatch=True\n",
    "            else:\n",
    "                needToPatch=True\n",
    "    \n",
    "            if needToPatch:\n",
    "                #print(MetaData)      \n",
    "                MDR=requests.put(url + \"samples/\"+id+\"/meta\", headers = headers2,data = MetaData)\n",
    "                ####check the MetaData loading was correct\n",
    "                if MDR.status_code not in [200,204]:\n",
    "                    print(\"error for \" + name + \" for feature \" + fea)\n",
    "                    print(MDR.status_code)\n",
    "                    print(MDR.raise_for_status())\n",
    "                    break\n",
    "    #print(\"metadata OK for \"+ name + \" (\" + id + \")\")\n",
    "    ###patch the quantity\n",
    "    Quant={}\n",
    "    for fea in ['Amount','Unit']:\n",
    "        if SkeDict[fea].startswith(\"fixed\"):\n",
    "            element=SkeDict[fea].split(\"_\")[1]\n",
    "        elif SkeDict[fea]==\"None\":\n",
    "            element=\"Nothing entered\"\n",
    "        else:\n",
    "            element=table[SkeDict[fea]][index]\n",
    "        Quant[fea]=element\n",
    "    Quant[\"displayUnit\"]=Quant[\"Unit\"].capitalize()\n",
    "    Quant[\"fullAmount\"]=Quant[\"Amount\"]\n",
    "    QR=requests.put(url + \"samples/\" + id + \"/quantity\", headers = headers2, data = Quant)\n",
    "    if QR.status_code not in [200,204]:\n",
    "        print(\"error for \" + name + \" for quantity\")\n",
    "        print(QR.status_code)\n",
    "        print(QR.raise_for_status())\n",
    "    #else:\n",
    "    #    print(\"quantity OK for \"+ name + \" (\" + id + \")\")\n",
    "    \n",
    "    #if index > 9:\n",
    "    #    print(\"break after 10\")\n",
    "    #    break \n",
    "print(\"finished\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71775041",
   "metadata": {},
   "source": [
    "### For Extracts\n",
    "We start from DDBB_extracts.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6635e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Extract\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "FeateLabExe = {}\n",
    "for feat in ['Name','Description','Note','Amount','Unit',\"parentSampleID\"]:\n",
    "    FeateLabExe[feat] = {\"ID\": \"notMeta\"}\n",
    "for feat in data.get(\"data\"):\n",
    "    FeateLabExe[format(feat.get(\"key\"))] = { \"ID\":format(feat.get(\"sampleTypeMetaID\")),\n",
    "                                              \"TYPE\":format(feat.get(\"sampleDataType\"))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c36bf",
   "metadata": {},
   "source": [
    "And check that they have been declared ExeDict.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c18ccfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in FeateLabExe.keys():\n",
    "    if feat not in ExeDict.keys():\n",
    "        print(feat + \"--> NOT IN DICTIONARY\")\n",
    "        \n",
    "for feat in ExeDict.keys():\n",
    "    if feat not in FeateLabExe.keys():\n",
    "        print(feat + \"--> NOT IN eLAB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa9dd4",
   "metadata": {},
   "source": [
    "We get all the possible values for checkboxes and dropdown features of Extracts and check our extractTable table is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c57b721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Extract\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "for feat in data.get(\"data\"):\n",
    "    if feat.get(\"sampleDataType\") == \"CHECKBOX\" or feat.get(\"sampleDataType\") == \"COMBO\":\n",
    "        OptionELAB=feat.get(\"optionValues\")\n",
    "        key=feat.get(\"key\")\n",
    "        if ExeDict[key].startswith(\"fixed\"):\n",
    "            tabVal=ExeDict[key].split(\"_\")[1]\n",
    "            if tabVal not in OptionELAB:\n",
    "                print(\"--\" + tabVal + \"-- not mapped in eLab for \" + key)\n",
    "        else:\n",
    "            extractTable.loc[extractTable[ExeDict[key]].isnull(),ExeDict[key]]=\"NA\"\n",
    "            for tabVal in extractTable[ExeDict[key]].unique():\n",
    "                if tabVal not in OptionELAB:\n",
    "                    print(\"--\" + tabVal + \"-- not mapped in eLab for \" + key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da718606",
   "metadata": {},
   "source": [
    "Now, we make the json for each extract and we upload or update in eLab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "753567b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference for AR0248.2.02(feature: density UDG treatment (ng/uL)) Nothing entered vs loaded : 0.258\n",
      "replace y/n??n\n",
      "difference for AR0248.2.02(feature: Volume UDG treatment (uL)) Nothing entered vs loaded : 22\n",
      "replace y/n??n\n",
      "difference for AR0248.2.02(feature: mass UDG in Tube (ng)) Nothing entered vs loaded : 5.676\n",
      "replace y/n??n\n",
      "difference for AR0248.2.02(feature: Extracted) NA vs loaded : yes\n",
      "replace y/n??n\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "###iterate over extracts\n",
    "for index,name in extractTable[ExeDict['Name']].items():\n",
    "    #print(str(index)+\" \"+name)\n",
    "    ####load the Data!\n",
    "    Data={}\n",
    "    for fea in FeateLabExe.keys():\n",
    "        if FeateLabExe[fea]['ID'] == \"notMeta\":\n",
    "            ###fixed value (from dico)\n",
    "            if ExeDict[fea].startswith(\"fixed\"):\n",
    "                element=ExeDict[fea].split(\"_\")[1]\n",
    "            elif ExeDict[fea]==\"None\":\n",
    "                element=\"Nothing entered\"\n",
    "            elif fea == \"parentSampleID\":\n",
    "                if not name.startswith(\"Blank\"):\n",
    "                    element=registered[\"Skeleton Element\"][extractTable[\"RascovanLabID\"][index]]\n",
    "                else:\n",
    "                    element=None\n",
    "            else:\n",
    "                element=extractTable[ExeDict[fea]][index]\n",
    "            Data[fea]=element\n",
    "    ###case of updating\n",
    "    if name in registered['Extract'].keys():\n",
    "        #print(name + \"updating\")\n",
    "        patch=True\n",
    "        id=registered['Extract'][name]\n",
    "        Data[\"Note\"]=\"Updated from API\"\n",
    "        ###QUERY CAMPO\n",
    "        ##SI CAMPO ELAB <> CAMPO TABLA:\n",
    "        ##        proimpt: update???\n",
    "            \n",
    "        DR=requests.patch(url + \"samples/\"+id, headers = headers2,data = Data)\n",
    "    else:\n",
    "        ###case of uploading\n",
    "        #print(name + \"uploading\")\n",
    "        patch=False\n",
    "        Data[\"Note\"]=\"Uploaded from API\"\n",
    "        Data[\"sampleTypeID\"]=types[\"Extract\"]\n",
    "        Data[\"Name\"]=name\n",
    "        DR=requests.post(url + \"samples/\", headers = headers2,data = Data)             \n",
    "    ####check the Data loading was correct\n",
    "    if DR.status_code not in [200,204]:\n",
    "        print(\"error for \" + name)\n",
    "        print(DR.status_code)\n",
    "        print(DR.raise_for_status())\n",
    "    ###actualize the registered[\"Site\"] list (checking we did not duplicated anything here)\n",
    "    r=requests.get(url + \"samples/forNames?names=\"+name, headers = headers2)\n",
    "    data=r.json()\n",
    "    sam=data.get(\"data\")\n",
    "    if len(sam)!=1:\n",
    "        print(\"different Extract entries (\" + str(len(sam)) + \") for name \"+name)\n",
    "        break\n",
    "    else:\n",
    "        sam=sam[0]\n",
    "        id=str(sam.get(\"sampleID\"))\n",
    "        #print(\"Data OK for \"+ name + \" (\" + id + \")\")\n",
    "        registered[\"Extract\"][name]=id\n",
    "\n",
    "    ###patch the metaData\n",
    "    if patch:\n",
    "        #print(\"patching meta so need to heck if differences for \"+name)\n",
    "        MDR=requests.get(url + \"samples/\"+id+\"/meta\", headers = headers2)\n",
    "        if MDR.status_code!=200:\n",
    "            print(\"error querrying meta for \" + name)\n",
    "            break\n",
    "        data=MDR.json().get(\"data\")\n",
    "        metaLoaded={}\n",
    "        for i in data:\n",
    "            metaLoaded[i[\"key\"]]=str(i[\"value\"])\n",
    "\n",
    "    for fea in FeateLabExe.keys():\n",
    "        needToPatch=False\n",
    "        MDR=requests.get(url + \"samples/\"+id+\"/meta\", headers = headers2)\n",
    "        ###get new element to be loaded\n",
    "        if FeateLabExe[fea]['ID'] != \"notMeta\" and FeateLabExe[fea]['TYPE'] != \"FILE\":\n",
    "            ###fixed value (from dico)\n",
    "            if ExeDict[fea].startswith(\"fixed\"):\n",
    "                element=ExeDict[fea].split(\"_\")[1]\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabExe[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabExe[fea]['TYPE']}\n",
    "            elif ExeDict[fea]==\"None\":\n",
    "                element=\"Nothing entered\"\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabExe[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabExe[fea]['TYPE']}\n",
    "            elif fea == \"From Skeleton Element\":\n",
    "                if not name.startswith(\"Blank\"):\n",
    "                    sisi=extractTable[\"RascovanLabID\"][index]\n",
    "                    IDsisi=registered[\"Skeleton Element\"][sisi]\n",
    "                    element=sisi+\"|\"+IDsisi\n",
    "                    samples={\"sampleID\": IDsisi,\"name\": sisi}\n",
    "                else:\n",
    "                    samples=[]\n",
    "                    splitted=extractTable[\"extractionComment\"][index].split(\",\")\n",
    "                    splitted=list(dict.fromkeys(splitted))\n",
    "                    for sisi in splitted:\n",
    "                        IDsisi=registered[\"Extract\"][sisi]\n",
    "                        samples.append({\"sampleID\": IDsisi,\"name\": sisi})\n",
    "                        if sisi != splitted[0]:\n",
    "                            element=element+\"|\"+sisi+\"|\"+IDsisi\n",
    "                        else:\n",
    "                            element=sisi+\"|\"+IDsisi\n",
    "                MetaData={\n",
    "                    \"sampleTypeMetaID\": int(FeateLabExe[fea]['ID']),\n",
    "                    \"sampleDataType\": FeateLabExe[fea]['TYPE'],\n",
    "                    \"samples\": samples,\n",
    "                    \"key\": fea,\n",
    "                    \"value\": element\n",
    "                }\n",
    "            else:\n",
    "                element=extractTable[ExeDict[fea]][index]\n",
    "                if format(element)==\"nan\" or format(element)==\"\" or format(element)==\" \":\n",
    "                    element=\"Nothing entered\"\n",
    "                MetaData={\"key\": fea,\n",
    "                          \"sampleTypeMetaID\": int(FeateLabExe[fea]['ID']),\n",
    "                          \"value\": element,\n",
    "                          \"sampleDataType\": FeateLabExe[fea]['TYPE']}\n",
    "            \n",
    "            ###check if this is a new entry or not\n",
    "            if patch:\n",
    "                ###check if new element is similar to what already loaded\n",
    "                if fea not in metaLoaded.keys(): \n",
    "                    needToPatch=True\n",
    "                elif metaLoaded[fea] != str(element):\n",
    "                    print(\"difference for \" + name + \"(feature: \" + fea + \") \" + element + \" vs loaded : \" + metaLoaded[fea])\n",
    "                    #prompt=\"y\"\n",
    "                    prompt=\"?\"\n",
    "                    while prompt not in [\"y\",\"n\"]:\n",
    "                        prompt = input(\"replace y/n??\")\n",
    "                    if prompt == \"y\":\n",
    "                        needToPatch=True\n",
    "            else:\n",
    "                needToPatch=True\n",
    "    \n",
    "            if needToPatch:\n",
    "                #print(MetaData)      \n",
    "                MDR=requests.put(url + \"samples/\"+id+\"/meta\", headers = headers2,data = MetaData)\n",
    "                ####check the MetaData loading was correct\n",
    "                if MDR.status_code not in [200,204]:\n",
    "                    print(\"error for \" + name + \" for feature \" + fea)\n",
    "                    print(MDR.status_code)\n",
    "                    print(MDR.raise_for_status())\n",
    "                    break\n",
    "    #print(\"metadata OK for \"+ name + \" (\" + id + \")\")\n",
    "    ###patch the quantity\n",
    "    Quant={}\n",
    "    Note=None\n",
    "    for fea in ['Amount','Unit']:\n",
    "        if ExeDict[fea].startswith(\"fixed\"):\n",
    "            element=ExeDict[fea].split(\"_\")[1]\n",
    "        elif ExeDict[fea]==\"None\":\n",
    "            element=\"Nothing entered\"\n",
    "        else:\n",
    "            element=extractTable[ExeDict[fea]][index]\n",
    "            if format(element)==\"nan\":\n",
    "                element=0\n",
    "            elif \"<\" in element:\n",
    "                Note=\"actual weight reported: \"+element\n",
    "                element=0\n",
    "        Quant[fea]=element\n",
    "    Quant[\"displayUnit\"]=Quant[\"Unit\"].capitalize()\n",
    "    Quant[\"fullAmount\"]=Quant[\"Amount\"]\n",
    "    QR=requests.put(url + \"samples/\" + id + \"/quantity\", headers = headers2, data = Quant)\n",
    "    if QR.status_code not in [200,204]:\n",
    "        print(\"error for \" + name + \" for quantity\")\n",
    "        print(QR.status_code)\n",
    "        print(QR.raise_for_status())\n",
    "    ###put actual weight in note when there is a \"<\"\n",
    "    if Note is not None:\n",
    "            r=requests.get(url + \"samples/\"+id, headers = headers2)\n",
    "            if r.status_code not in [200,204]:\n",
    "                print(\"error for \" + name + \" for quantity 2\")\n",
    "            Data=r.json()\n",
    "            Data[\"note\"]=Data[\"note\"]+\" / \"+ Note\n",
    "            r=requests.patch(url + \"samples/\"+id, headers = headers2,data = Data)\n",
    "            if r.status_code not in [200,204]:\n",
    "                print(\"error for \" + name + \" for quantity 3\")\n",
    "print(\"finished\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c10b9",
   "metadata": {},
   "source": [
    "### For Non indexed librairies\n",
    "We have not done any external tables. I just actualize the parent sample link!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "811e67c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': {'ID': 'notMeta'}, 'Description': {'ID': 'notMeta'}, 'Note': {'ID': 'notMeta'}, 'Amount': {'ID': 'notMeta'}, 'Unit': {'ID': 'notMeta'}, 'parentSampleID': {'ID': 'notMeta'}, 'Person in charge': {'ID': '245169', 'TYPE': 'CHECKBOX'}, 'Date of preparation': {'ID': '245170', 'TYPE': 'DATE'}, 'Labroatory where prepared': {'ID': '245171', 'TYPE': 'CHECKBOX'}, 'UDG treatment type': {'ID': '245172', 'TYPE': 'COMBO'}, 'SSB and Adapters dilution type': {'ID': '245173', 'TYPE': 'COMBO'}, 'Temporary UDG treated extract location': {'ID': '245174', 'TYPE': 'TEXT'}, 'Initial Volume (uL)': {'ID': '245175', 'TYPE': 'NUMERIC'}, 'Remaining Volume (uL)': {'ID': '245176', 'TYPE': 'NUMERIC'}, 'From Extract': {'ID': '245177', 'TYPE': 'SAMPLELINK'}}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Non Indexed Library\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "FeateLabNiLib = {}\n",
    "for feat in ['Name','Description','Note','Amount','Unit',\"parentSampleID\"]:\n",
    "    FeateLabNiLib[feat] = {\"ID\": \"notMeta\"}\n",
    "for feat in data.get(\"data\"):\n",
    "    FeateLabNiLib[format(feat.get(\"key\"))] = { \"ID\":format(feat.get(\"sampleTypeMetaID\")),\n",
    "                                              \"TYPE\":format(feat.get(\"sampleDataType\"))}\n",
    "print(FeateLabNiLib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b5cc32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###GET extract Name from library name\n",
    "def getExtract(libName):\n",
    "    ret=libName[0]\n",
    "    for char in libName[1:(len(libName)-2)]:\n",
    "        ret=ret+char\n",
    "    return(ret)\n",
    "\n",
    "\n",
    "for niLib in registered[\"Non Indexed Library\"].keys():\n",
    "    #print(niLib)\n",
    "    id=registered[\"Non Indexed Library\"][niLib]\n",
    "    #retrieve data\n",
    "    DR=requests.get(url + \"samples/\"+id, headers = headers2)\n",
    "    if DR.status_code not in [200]:\n",
    "        print(\"error retrieving \" + niLib)\n",
    "        print(DR.status_code)\n",
    "        print(DR.raise_for_status())\n",
    "        break\n",
    "    Data=DR.json()\n",
    "    extract=None\n",
    "    idExtract=None\n",
    "    if niLib.startswith(\"BL\"):\n",
    "        MDR=requests.get(url + \"samples/\"+id+\"/meta/\", headers = headers2)\n",
    "        ####check the MetaData loading was correct\n",
    "        if MDR.status_code not in [200,204]:\n",
    "            print(\"error retrieving from extract for \" + niLib)\n",
    "            print(MDR.status_code)\n",
    "            print(MDR.raise_for_status())\n",
    "            break\n",
    "        data=MDR.json().get(\"data\")\n",
    "        for dd in data:\n",
    "            if dd[\"key\"] == \"From Extract\":\n",
    "                extract=dd[\"value\"].split(\"|\")[0]\n",
    "    else:\n",
    "        extract=getExtract(niLib)\n",
    "    if extract is None:\n",
    "        print(\"extract not retrieve\")\n",
    "        break\n",
    "    idExtract=registered[\"Extract\"][extract]\n",
    "    \n",
    "    ###parent sample:\n",
    "    Data[\"parentSampleID\"]=idExtract\n",
    "    DR=requests.patch(url + \"samples/\"+id, headers = headers2,data=Data)\n",
    "    if DR.status_code not in [200,204]:\n",
    "        print(\"error patching\" + niLib)\n",
    "        print(DR.status_code)\n",
    "        print(DR.raise_for_status())\n",
    "        break\n",
    "        \n",
    "    ###From extract\n",
    "    element=extract+\"|\"+idExtract\n",
    "    samples={\"sampleID\": idExtract,\"name\": extract}\n",
    "    MetaData={\n",
    "        \"sampleTypeMetaID\": int(FeateLabNiLib[\"From Extract\"]['ID']),\n",
    "        \"sampleDataType\": FeateLabNiLib[\"From Extract\"]['TYPE'],\n",
    "        \"samples\": samples,\n",
    "        \"key\": \"From Extract\",\n",
    "        \"value\": element\n",
    "    }\n",
    "    MDR=requests.put(url + \"samples/\"+id+\"/meta\", headers = headers2,data = MetaData)\n",
    "    ####check the MetaData loading was correct\n",
    "    if MDR.status_code not in [200,204]:\n",
    "        print(\"error retrieving meta for \" + niLib)\n",
    "        print(MDR.status_code)\n",
    "        print(MDR.raise_for_status())\n",
    "        break\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0eaae5",
   "metadata": {},
   "source": [
    "### Indexed library\n",
    "Same: we just actualize the links to Non Indexed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fe47136",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': {'ID': 'notMeta'}, 'Description': {'ID': 'notMeta'}, 'Note': {'ID': 'notMeta'}, 'Amount': {'ID': 'notMeta'}, 'Unit': {'ID': 'notMeta'}, 'parentSampleID': {'ID': 'notMeta'}, 'Person in charge': {'ID': '244383', 'TYPE': 'CHECKBOX'}, 'From Non Indexed Library': {'ID': '244384', 'TYPE': 'SAMPLELINK'}, 'Laboratory where processed': {'ID': '244385', 'TYPE': 'CHECKBOX'}, 'Date of preparation': {'ID': '245107', 'TYPE': 'DATE'}, 'qPCR': {'ID': '245114', 'TYPE': 'COMBO'}, 'number of Ct': {'ID': '245115', 'TYPE': 'NUMERIC'}, 'Date of qPCR': {'ID': '245116', 'TYPE': 'DATE'}, 'qPCR Comment ': {'ID': '245117', 'TYPE': 'TEXTAREA'}, 'Dual Unique Index ID': {'ID': '245118', 'TYPE': 'TEXT'}, 'Indexing PCR (# cycles)': {'ID': '245119', 'TYPE': 'NUMERIC'}, 'Density QC-1 nmol/L': {'ID': '245121', 'TYPE': 'NUMERIC'}, 'Number of QC performed': {'ID': '245122', 'TYPE': 'COMBO'}, 'Density QC-1 ng/uL': {'ID': '245123', 'TYPE': 'NUMERIC'}, 'QC-1 Elution Volume of Library (uL)': {'ID': '245124', 'TYPE': 'NUMERIC'}, 'QC-1 Average Read Length (bp)': {'ID': '245125', 'TYPE': 'NUMERIC'}, 'Library Profile': {'ID': '245126', 'TYPE': 'FILE'}, 'QC-1 date': {'ID': '245127', 'TYPE': 'DATE'}, 'Volume taken for indexing (uL)': {'ID': '245166', 'TYPE': 'NUMERIC'}, 'Indexing PCR date': {'ID': '245167', 'TYPE': 'DATE'}, 'Sample well in library profile file': {'ID': '245182', 'TYPE': 'TEXT'}}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url + \"sampleTypes/\" + types[\"Indexed Library\"] + \"/meta\", headers = headers2)\n",
    "data = r.json()\n",
    "FeateLabILib = {}\n",
    "for feat in ['Name','Description','Note','Amount','Unit',\"parentSampleID\"]:\n",
    "    FeateLabILib[feat] = {\"ID\": \"notMeta\"}\n",
    "for feat in data.get(\"data\"):\n",
    "    FeateLabILib[format(feat.get(\"key\"))] = { \"ID\":format(feat.get(\"sampleTypeMetaID\")),\n",
    "                                              \"TYPE\":format(feat.get(\"sampleDataType\"))}\n",
    "print(FeateLabILib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7da8f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "for ILib in registered[\"Indexed Library\"].keys():\n",
    "    #print(ILib)\n",
    "    id=registered[\"Indexed Library\"][ILib]\n",
    "    #retrieve data\n",
    "    DR=requests.get(url + \"samples/\"+id, headers = headers2)\n",
    "    if DR.status_code not in [200]:\n",
    "        print(\"error retrieving \" + niLib)\n",
    "        print(DR.status_code)\n",
    "        print(DR.raise_for_status())\n",
    "        break\n",
    "    Data=DR.json()\n",
    "    niLib=None\n",
    "    idniLib=None\n",
    "    MDR=requests.get(url + \"samples/\"+id+\"/meta/\", headers = headers2)\n",
    "    ####check the MetaData loading was correct\n",
    "    if MDR.status_code not in [200,204]:\n",
    "        print(\"error retrieving from extract for \" + ILib)\n",
    "        print(MDR.status_code)\n",
    "        print(MDR.raise_for_status())\n",
    "        break\n",
    "    data=MDR.json().get(\"data\")\n",
    "    for dd in data:\n",
    "        if dd[\"key\"] == \"From Non Indexed Library\":\n",
    "            niLib=dd[\"value\"].split(\"|\")[0]\n",
    "    if niLib is None:\n",
    "        print(\"Non indexed Library not retrieve\")\n",
    "        break\n",
    "    idniLib=registered[\"Non Indexed Library\"][niLib]\n",
    "    \n",
    "    ###parent sample:\n",
    "    Data[\"parentSampleID\"]=idniLib\n",
    "    DR=requests.patch(url + \"samples/\"+id, headers = headers2,data=Data)\n",
    "    if DR.status_code not in [200,204]:\n",
    "        print(\"error patching\" + ILib)\n",
    "        print(DR.status_code)\n",
    "        print(DR.raise_for_status())\n",
    "        break\n",
    "        \n",
    "    ###From extract\n",
    "    element=niLib+\"|\"+idniLib\n",
    "    samples={\"sampleID\": idniLib,\"name\": niLib}\n",
    "    MetaData={\n",
    "        \"sampleTypeMetaID\": int(FeateLabILib[\"From Non Indexed Library\"]['ID']),\n",
    "        \"sampleDataType\": FeateLabILib[\"From Non Indexed Library\"]['TYPE'],\n",
    "        \"samples\": samples,\n",
    "        \"key\": \"From Non Indexed Library\",\n",
    "        \"value\": element\n",
    "    }\n",
    "    MDR=requests.put(url + \"samples/\"+id+\"/meta\", headers = headers2,data = MetaData)\n",
    "    ####check the MetaData loading was correct\n",
    "    if MDR.status_code not in [200,204]:\n",
    "        print(\"error retrieving meta for \" + ILib)\n",
    "        print(MDR.status_code)\n",
    "        print(MDR.raise_for_status())\n",
    "        break\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b39cf5",
   "metadata": {},
   "source": [
    "## Sample assignation to Experiments\n",
    "\n",
    "first retrieve the eLab ID needed to access the sampleIN and sampleOUT sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f99134c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(url + \"experiments\", headers = headers2,params = params)\n",
    "data = r.json()\n",
    "experiments = {}\n",
    "for exp in data.get(\"data\"):\n",
    "    experiments[format(exp.get(\"name\"))] = format(exp.get(\"experimentID\"))\n",
    "\n",
    "\n",
    "\n",
    "for expe in list(experiments.keys()):\n",
    "    #print(expe)\n",
    "    idExpe=experiments[expe]\n",
    "    r=requests.get(\"https://elab-dev.pasteur.fr/api/v1/experiments/\"+idExpe+\"/sections\",headers=headers1)\n",
    "    if r.status_code != 200:\n",
    "        print(r.status_code)\n",
    "        print(r.raise_for_status())\n",
    "    if r.json().get(\"recordCount\") == 0:\n",
    "        print(\"no record\")\n",
    "        continue\n",
    "    SampleIN={}\n",
    "    SampleOUT={}\n",
    "    for data in r.json().get(\"data\"):\n",
    "        if data[\"sectionType\"] == \"SAMPLESIN\":\n",
    "            SampleIN[data[\"sectionHeader\"]]=data[\"expJournalID\"]\n",
    "        elif data[\"sectionType\"] == \"SAMPLESOUT\":\n",
    "            SampleOUT[data[\"sectionHeader\"]]=data[\"expJournalID\"]\n",
    "    experiments[expe]={\"ID\":idExpe,\n",
    "                      \"sampleIN\":SampleIN,\n",
    "                      \"sampleOUT\":SampleOUT}\n",
    "#print(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc278720",
   "metadata": {},
   "source": [
    "### Assign to Labelling sampleIN the individuals and to Labelling sampleOUT the skeleton elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9c18fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleOUT=experiments[\"Labelling process\"][\"sampleOUT\"][\"Labelled Skeleton elements \"]\n",
    "sampleIN=experiments[\"Labelling process\"][\"sampleIN\"][\"Individuals labelled\"]\n",
    "\n",
    "listOUT=[]\n",
    "listIN=[]\n",
    "for inName in registered[\"Skeleton Element\"].keys():\n",
    "    inID=registered[\"Skeleton Element\"][inName]\n",
    "    listOUT.append(inID)\n",
    "    ###get the parent individual\n",
    "    r=requests.get(url+\"/samples/\"+inID+\"/parent\",headers=headers1)\n",
    "    if r.status_code !=200:\n",
    "        print(r.status_code)\n",
    "        r.raise_for_status()\n",
    "        break\n",
    "    rjson=r.json()\n",
    "    outName=rjson.get(\"name\")\n",
    "    listIN.append(rjson.get(\"sampleID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "280a86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOUT=format(listOUT)\n",
    "#print(listOUT)\n",
    "r=requests.put(url+\"/experiments/sections/\"+format(sampleOUT)+\"/samples\",headers=headers1,data = listOUT)\n",
    "if r.status_code !=204:\n",
    "    print(r.status_code)\n",
    "    r.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a98a27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "listIN=format(listIN)\n",
    "r=requests.put(url+\"/experiments/sections/\"+format(sampleIN)+\"/samples\",headers=headers1,data = listIN)\n",
    "if r.status_code !=204:\n",
    "    print(r.status_code)\n",
    "    r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724586f",
   "metadata": {},
   "source": [
    "### Assign to Drilling Rasovan Laboratory Protocols\n",
    "\"pulverized pieces\" (sampleIN) and  the skeleton elements they derive from (sampleOUT), all extracts that appear in \"DDBB_extract.csv\"\n",
    "\n",
    "\n",
    "Starts with retrieving all field IDs required for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5361c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sampleIN and sampleOUT id for the experiment\n",
    "CorresExtract={\"petrous\":\"Pulverized petrous bone\",\n",
    "                  \"dental calculus\":\"Scratched Dental Calculus\",\n",
    "                  \"pulp\":\"Pulverized Pulp\",\n",
    "                  \"root\":\"Pulverized Root\",\n",
    "                   \"root apex\":\"Pulverized Root Apex\",\n",
    "                  \"long bone\":\"Pulverized long bone\",\n",
    "                    \"other\":\"Pulverized other bone\",\n",
    "              }\n",
    "CorresSkel={\"Petrous\":\"Petrous bone processed\",\n",
    "            \"Tooth\":\"Tooth processed\",\n",
    "            \"Other Bone\":\"Long bone processed \"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d6757",
   "metadata": {},
   "source": [
    "Now we get extract and skeleton element ID and we assign them to different experiment (according to some in-house conditions:\n",
    " - name starting with NR means they were processed by Nico at Schroeder lab\n",
    " - expediente is \"Guareib / Pulverized\" means we already received the pulverized pieces\n",
    " - the skeleton element is Dental calculus means Mariano del Papa sent us the scratched dental calculus\n",
    " - else it it our own protocoles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "166d08f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Guraeib': {'Petrous bone processed': [], 'Tooth processed': ['9519957', '9519957', '9519958', '9519959', '9519960', '9519961', '9519962', '9519962', '9519963', '9519964', '9519964'], 'Long bone processed ': ['9519955', '9519955', '9519956', '9519956']}, 'Del Papa': {'Petrous bone processed': [], 'Tooth processed': [], 'Long bone processed ': []}, 'Schroeder': {'Petrous bone processed': ['9520367', '9520372', '9520375', '9520377', '9520379', '9520380', '9520381', '9520382', '9520383', '9520384', '9520385', '9520386', '9520387', '9520388', '9520389', '9520390', '9520391'], 'Tooth processed': ['9520368', '9520369', '9520370', '9520371'], 'Long bone processed ': []}, 'Rascovan': {'Petrous bone processed': ['9519919', '9519919', '9519921', '9519921', '9519921', '9519923', '9519923', '9519924', '9519924', '9519925', '9519925', '9519928', '9519928', '9520182', '9520182', '9520243', '9520243'], 'Tooth processed': ['9519935', '9519935', '9519935', '9519935', '9519941', '9519941', '9519941', '9519941', '9519943', '9519943', '9519943', '9519945', '9519945', '9519945', '9519947', '9519947', '9519947', '9519949', '9519949', '9519949', '9519949', '9519951', '9519951', '9519951', '9519951', '9519953', '9519953', '9519953', '9519953', '9519968', '9519968', '9519968', '9519968', '9519969', '9519970', '9519970', '9519970', '9519970', '9519971', '9519971', '9519971', '9519971', '9519971', '9519973', '9519976', '9519977', '9519977', '9519977', '9519980', '9519980', '9519996', '9519996', '9520000', '9520000', '9520006', '9520006', '9520076', '9520076', '9520077', '9520077', '9520077', '9520077', '9520085', '9520085', '9520085', '9520085', '9520085', '9520086', '9520086', '9520086', '9520086', '9520088', '9520088', '9520089', '9520089', '9520090', '9520095', '9520095', '9520098', '9520098', '9520109', '9520109', '9520110', '9520110', '9520111', '9520111', '9520112', '9520112', '9520113', '9520113', '9520114', '9520114', '9520114', '9520114', '9520114', '9520116', '9520116', '9520124', '9520124', '9520124', '9520127', '9520127', '9520197', '9520197', '9520241', '9520248', '9520248', '9520254', '9520254', '9520254', '9520254', '9520256', '9520256', '9520260', '9520260', '9520266', '9520266', '9520266', '9520276', '9520276', '9520276', '9520276', '9520299', '9520299', '9520299', '9520299', '9520310', '9520310', '9520310', '9520310', '9520311', '9520311', '9520314', '9520314', '9520314', '9520314', '9520322', '9520322', '9520322', '9520322', '9520328', '9520340', '9520340', '9520340', '9520340', '9520356', '9520356', '9520356', '9520356'], 'Long bone processed ': []}}\n",
      "{'Guraeib': {'Pulverized petrous bone': [], 'Scratched Dental Calculus': [], 'Pulverized Pulp': [], 'Pulverized Root': ['9520825', '9520826', '9520827', '9520828', '9520829', '9520830', '9520831', '9520832', '9520833', '9520834', '9520835'], 'Pulverized Root Apex': [], 'Pulverized long bone': ['9520821', '9520822', '9520823', '9520824'], 'Pulverized other bone': []}, 'Del Papa': {'Pulverized petrous bone': [], 'Scratched Dental Calculus': ['9520850', '9520851', '9520852', '9520853', '9520854', '9520855', '9520856', '9520857', '9520858', '9523926'], 'Pulverized Pulp': [], 'Pulverized Root': [], 'Pulverized Root Apex': [], 'Pulverized long bone': [], 'Pulverized other bone': []}, 'Schroeder': {'Pulverized petrous bone': ['9520924', '9520929', '9520930', '9520931', '9520932', '9520933', '9520934', '9520935', '9520936', '9520937', '9520938', '9520939', '9520940', '9520941', '9520942', '9520943', '9520944'], 'Scratched Dental Calculus': [], 'Pulverized Pulp': [], 'Pulverized Root': ['9520925', '9520926', '9520927', '9520928'], 'Pulverized Root Apex': [], 'Pulverized long bone': [], 'Pulverized other bone': []}, 'Rascovan': {'Pulverized petrous bone': ['9520779', '9520780', '9520781', '9520782', '9520783', '9520784', '9520785', '9520786', '9520787', '9520788', '9520789', '9520790', '9516598', '9520883', '9516615', '9520884', '9520885'], 'Scratched Dental Calculus': ['9520791', '9520795', '9520803', '9520809', '9520813', '9520817', '9520836', '9520841', '9520845', '9520849', '9523918', '9523920', '9523922', '9523924', '9523927', '9520860', '9520864', '9520869', '9523928', '9523930', '9523933', '9523935', '9523937', '9523939', '9523941', '9523943', '9520875', '9523945', '9523947', '9523949', '9523952', '9520886', '9523954', '9523956', '9520890', '9520893', '9520897', '9520901', '9520907', '9520911', '9520916', '9520920'], 'Pulverized Pulp': ['9520794', '9520799', '9520802', '9520808', '9520812', '9520816', '9520820', '9520839', '9520840', '9520844', '9520848', '9523913', '9520863', '9520868', '9520872', '9520874', '9520879', '9520882', '9520889', '9520896', '9520900', '9520904', '9520910', '9520914', '9520919', '9520923'], 'Pulverized Root': ['9520792', '9520793', '9520796', '9520798', '9520800', '9520801', '9520804', '9520805', '9520806', '9520807', '9520810', '9520811', '9520814', '9520815', '9520818', '9520819', '9520837', '9520838', '9520842', '9520843', '9520846', '9520847', '9523916', '9520861', '9520862', '9520865', '9520866', '9520867', '9520870', '9520871', '9520873', '9520876', '9520877', '9520880', '9520881', '9520887', '9520888', '9520891', '9520892', '9520894', '9520895', '9520898', '9520899', '9520902', '9520903', '9520905', '9520906', '9520908', '9520909', '9520912', '9520913', '9520915', '9520917', '9520918', '9520921', '9520922'], 'Pulverized Root Apex': ['9523914', '9523915', '9523917', '9523919', '9523921', '9523923', '9523925', '9520859', '9523929', '9523931', '9523932', '9523934', '9523936', '9523938', '9523940', '9523942', '9523944', '9523946', '9523948', '9523950', '9523951', '9523953', '9523955', '9523957'], 'Pulverized long bone': [], 'Pulverized other bone': ['9520878']}}\n"
     ]
    }
   ],
   "source": [
    "listOUT={}\n",
    "listIN={}\n",
    "for lab in [\"Guraeib\",\"Del Papa\",\"Schroeder\",\"Rascovan\"]:\n",
    "    listOUT[lab]={}\n",
    "    listIN[lab]={}\n",
    "    for exType in CorresExtract:\n",
    "        listOUT[lab][CorresExtract[exType]]=[]\n",
    "\n",
    "    for skelType in CorresSkel:\n",
    "        listIN[lab][CorresSkel[skelType]]=[]\n",
    "\n",
    "for index, extract in extractTable[\"ExtractID\"].items():\n",
    "    if extract.startswith(\"Blank\"):\n",
    "        continue\n",
    "    ###prepare sampleOUT for that extract\n",
    "    idOUT=registered[\"Extract\"][extract]\n",
    "    #get meta \n",
    "    MER=requests.get(url+\"/samples/\"+idOUT+\"/meta\",headers=headers1)\n",
    "    if MER.status_code !=200:\n",
    "        print(MER.raise_for_status())\n",
    "        break\n",
    "    #get Extract Type and check it is found\n",
    "    exType=None\n",
    "    for meta in MER.json().get(\"data\"):\n",
    "        if meta[\"key\"]==\"Extract Type\":\n",
    "            exType=meta[\"value\"]\n",
    "            break\n",
    "    if exType is None:\n",
    "        print(\"Extract Type not found\")\n",
    "        break\n",
    "    ###prepare sampleIN for that extract\n",
    "    #get parentSampleID (the skeleton element)\n",
    "    ER=requests.get(url+\"/samples/\"+idOUT,headers=headers1)\n",
    "    if ER.status_code !=200:\n",
    "        print(ER.raise_for_status())\n",
    "        break\n",
    "    idIN=format(ER.json()[\"parentSampleID\"])\n",
    "\n",
    "    #get meta\n",
    "    SMR=requests.get(url+\"/samples/\"+idIN+\"/meta\",headers=headers1)\n",
    "    if SMR.status_code !=200:\n",
    "        print(SMR.raise_for_status())\n",
    "        break\n",
    "    ##get skeleton element type and check it is found\n",
    "    archoID=None\n",
    "    skelType=None\n",
    "    expediente=None\n",
    "    for meta in SMR.json().get(\"data\"):\n",
    "        if meta[\"key\"]==\"Bone type\":\n",
    "            skelType=meta[\"value\"]\n",
    "        elif meta[\"key\"]==\"Exportation Permit Number\":\n",
    "            expediente=meta[\"value\"]\n",
    "        elif meta[\"key\"]==\"Archaeologist sample ID\":\n",
    "            archoID=meta[\"value\"]\n",
    "    if skelType is None:\n",
    "        print(\"Skeleton Ele Type not found\")\n",
    "        print(SMR.json().get(\"data\"))\n",
    "        break\n",
    "    if expediente is None:\n",
    "        print(\"Expediente not found\")\n",
    "        print(SMR.json().get(\"data\"))\n",
    "        break\n",
    "    if archoID is None:\n",
    "        print(\"archeo ID not found\")\n",
    "        print(SMR.json().get(\"data\"))\n",
    "        break\n",
    "\n",
    "    if skelType == \"Dental Calculus\":\n",
    "        #print(\"del Papa\")\n",
    "        listOUT[\"Del Papa\"][CorresExtract[exType]].append(idOUT)\n",
    "    elif expediente==\"Solana Guraeib / Pulverized\":\n",
    "        #print(\"Guareib\")\n",
    "        listOUT[\"Guraeib\"][CorresExtract[exType]].append(idOUT)\n",
    "        listIN[\"Guraeib\"][CorresSkel[skelType]].append(idIN)\n",
    "    elif archoID.startswith(\"NR\"):\n",
    "        #print(\"Schroeder\")\n",
    "        listOUT[\"Schroeder\"][CorresExtract[exType]].append(idOUT)\n",
    "        listIN[\"Schroeder\"][CorresSkel[skelType]].append(idIN)\n",
    "    else:\n",
    "        #print(\"Rascovan\")\n",
    "        listOUT[\"Rascovan\"][CorresExtract[exType]].append(idOUT)\n",
    "        listIN[\"Rascovan\"][CorresSkel[skelType]].append(idIN)\n",
    "        \n",
    "\n",
    "print(listIN)\n",
    "print(listOUT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f592aacf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample IN : nothing to upload upload for Petrous bone processed to Guraeib\n",
      "sample IN : upload for Tooth processed to Guraeib\n",
      "sample IN : upload for Long bone processed  to Guraeib\n",
      "sample IN : nothing to upload upload for Petrous bone processed to Del Papa\n",
      "sample IN : nothing to upload upload for Tooth processed to Del Papa\n",
      "sample IN : nothing to upload upload for Long bone processed  to Del Papa\n",
      "sample IN : upload for Petrous bone processed to Schroeder\n",
      "sample IN : upload for Tooth processed to Schroeder\n",
      "sample IN : nothing to upload upload for Long bone processed  to Schroeder\n",
      "sample IN : upload for Petrous bone processed to Rascovan\n",
      "sample IN : upload for Tooth processed to Rascovan\n",
      "sample IN : nothing to upload upload for Long bone processed  to Rascovan\n"
     ]
    }
   ],
   "source": [
    "###upload sample IN\n",
    "for lab in [\"Guraeib\",\"Del Papa\",\"Schroeder\",\"Rascovan\"]:\n",
    "    for type in listIN[lab].keys():\n",
    "        data=listIN[lab][type]\n",
    "        if len(data)==0:\n",
    "            print(\"sample IN : nothing to upload upload for \"+type+\" to \"+lab)\n",
    "        else:\n",
    "            idIN=format(experiments[\"Drilling. \"+lab+\" Laboratory Protocols\"][\"sampleIN\"][type])\n",
    "            print(\"sample IN : upload for \"+type+\" to \"+lab)\n",
    "            data=format(data)\n",
    "            r=requests.put(url+\"/experiments/sections/\"+idIN+\"/samples\",headers=headers1,data = data)\n",
    "            if r.status_code !=204:\n",
    "                print(r.status_code)\n",
    "                r.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "702725f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample OUT : nothing to upload upload for Pulverized petrous bone to Guraeib\n",
      "sample OUT : nothing to upload upload for Scratched Dental Calculus to Guraeib\n",
      "sample OUT : nothing to upload upload for Pulverized Pulp to Guraeib\n",
      "sample OUT : upload for Pulverized Root to Guraeib\n",
      "sample OUT : nothing to upload upload for Pulverized Root Apex to Guraeib\n",
      "sample OUT : upload for Pulverized long bone to Guraeib\n",
      "sample OUT : nothing to upload upload for Pulverized other bone to Guraeib\n",
      "sample OUT : nothing to upload upload for Pulverized petrous bone to Del Papa\n",
      "sample OUT : upload for Scratched Dental Calculus to Del Papa\n",
      "sample OUT : nothing to upload upload for Pulverized Pulp to Del Papa\n",
      "sample OUT : nothing to upload upload for Pulverized Root to Del Papa\n",
      "sample OUT : nothing to upload upload for Pulverized Root Apex to Del Papa\n",
      "sample OUT : nothing to upload upload for Pulverized long bone to Del Papa\n",
      "sample OUT : nothing to upload upload for Pulverized other bone to Del Papa\n",
      "sample OUT : upload for Pulverized petrous bone to Schroeder\n",
      "sample OUT : nothing to upload upload for Scratched Dental Calculus to Schroeder\n",
      "sample OUT : nothing to upload upload for Pulverized Pulp to Schroeder\n",
      "sample OUT : upload for Pulverized Root to Schroeder\n",
      "sample OUT : nothing to upload upload for Pulverized Root Apex to Schroeder\n",
      "sample OUT : nothing to upload upload for Pulverized long bone to Schroeder\n",
      "sample OUT : nothing to upload upload for Pulverized other bone to Schroeder\n",
      "sample OUT : upload for Pulverized petrous bone to Rascovan\n",
      "sample OUT : upload for Scratched Dental Calculus to Rascovan\n",
      "sample OUT : upload for Pulverized Pulp to Rascovan\n",
      "sample OUT : upload for Pulverized Root to Rascovan\n",
      "sample OUT : upload for Pulverized Root Apex to Rascovan\n",
      "sample OUT : nothing to upload upload for Pulverized long bone to Rascovan\n",
      "sample OUT : upload for Pulverized other bone to Rascovan\n"
     ]
    }
   ],
   "source": [
    "###upload sample OUT\n",
    "for lab in [\"Guraeib\",\"Del Papa\",\"Schroeder\",\"Rascovan\"]:\n",
    "    for type in listOUT[lab].keys():\n",
    "        data=listOUT[lab][type]\n",
    "        if len(data)==0:\n",
    "            print(\"sample OUT : nothing to upload upload for \"+type+\" to \"+lab)\n",
    "        else:\n",
    "            idOUT=format(experiments[\"Drilling. \"+lab+\" Laboratory Protocols\"][\"sampleOUT\"][type])\n",
    "            print(\"sample OUT : upload for \"+type+\" to \"+lab)\n",
    "            data=format(data)\n",
    "            r=requests.put(url+\"/experiments/sections/\"+idOUT+\"/samples\",headers=headers1,data = data)\n",
    "            if r.status_code !=204:\n",
    "                print(r.status_code)\n",
    "                r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2539f",
   "metadata": {},
   "source": [
    "### Experiment: Extraction. Rascovan Laboratory Protocols\n",
    "Now we add in \"as sampleIN and sampleOUT the \"pulverized bone\" for which there is an non-indexed library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a1b27e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "listIN=[]\n",
    "for lib,libID in registered[\"Non Indexed Library\"].items():\n",
    "    r=requests.get(url+\"/samples/\"+libID,headers=headers1)\n",
    "    if r.status_code !=200:\n",
    "        print(r.status_code)\n",
    "        print(r.raise_for_status())\n",
    "        break\n",
    "    extID=r.json().get(\"parentSampleID\")\n",
    "    r=requests.get(url+\"/samples/\"+format(extID),headers=headers1)\n",
    "    if r.status_code !=200:\n",
    "        print(r.status_code)\n",
    "        print(r.raise_for_status())\n",
    "        break\n",
    "    ext=r.json().get(\"name\")\n",
    "    if ext.startswith(\"Blank\"):\n",
    "        date=\"\".join(ext.split(\".\")[1].split(\"-\")[::-1])\n",
    "        if lib !=\"BL\"+date+\"00\":\n",
    "            print(\"HU? \"+ext+\" for \"+lib)\n",
    "            break\n",
    "    elif ext+\"00\" != lib:\n",
    "        print(\"HU? \"+ext+\" for \"+lib)\n",
    "        print(ext)\n",
    "        break\n",
    "    listIN.append(extID)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f32b5905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5423444\n",
      "<Response [204]>\n",
      "5423445\n",
      "<Response [204]>\n"
     ]
    }
   ],
   "source": [
    "data=format(listIN)\n",
    "###assign to sampleIN\n",
    "idExp={\"c\":str(value) for key, value in experiments[\"Extraction. Rascovan Lab Protocols\"][\"sampleIN\"].items()}[\"c\"]\n",
    "print(idExp)\n",
    "r=requests.put(url+\"/experiments/sections/\"+idExp+\"/samples\",headers=headers1,data = data)\n",
    "print(r)\n",
    "###assign to sampleOUT\n",
    "idExp={\"c\":str(value) for key, value in experiments[\"Extraction. Rascovan Lab Protocols\"][\"sampleOUT\"].items()}[\"c\"]\n",
    "print(idExp)\n",
    "r=requests.put(url+\"/experiments/sections/\"+idExp+\"/samples\",headers=headers1,data = data)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654542f",
   "metadata": {},
   "source": [
    "### Experiment: Library Prep. Rascovan Lab protocols\n",
    "Now we add in \"as sampleIN \"pulverized bone\" for which there is an \"indexed library\" and as sampleOUT that indexed library and the corresponding non indexed library\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91c0cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "72\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "extIN=[]\n",
    "nonIndLibOUT=[]\n",
    "indLibOUT=[]\n",
    "for indLib,indLibID in registered[\"Indexed Library\"].items():\n",
    "    \n",
    "    ###retrieve the non Index Librar info\n",
    "    r=requests.get(url+\"/samples/\"+indLibID,headers=headers1)\n",
    "    if r.status_code !=200:\n",
    "        print(r.status_code)\n",
    "        print(r.raise_for_status())\n",
    "        break\n",
    "    nonIndLibID=r.json().get(\"parentSampleID\")\n",
    "    r=requests.get(url+\"/samples/\"+format(nonIndLibID),headers=headers1)\n",
    "    if r.status_code !=200:\n",
    "        print(r.status_code)\n",
    "        print(r.raise_for_status())\n",
    "        break\n",
    "    nonIndLib=r.json().get(\"name\")\n",
    "    ###check inIndLib ifinished with 0\n",
    "    if nonIndLib[-1] !=\"0\":\n",
    "        print(\"hu \"+nonIndLib)\n",
    "        break\n",
    "    ###check nonIndLib and IndLib corresponds\n",
    "    if ''.join(nonIndLib[0:(len(nonIndLib)-2)]) != ''.join(indLib[0:(len(indLib)-2)]):\n",
    "        print(\"HU? \"+indLib+\" \"+nonIndLib)\n",
    "        break\n",
    "        \n",
    "        \n",
    "    ###retrieve the extract \n",
    "    extID=r.json().get(\"parentSampleID\")\n",
    "    r=requests.get(url+\"/samples/\"+format(extID),headers=headers1)\n",
    "    if r.status_code !=200:\n",
    "        print(r.status_code)\n",
    "        print(r.raise_for_status())\n",
    "        break\n",
    "    ext=r.json().get(\"name\")\n",
    "    ##check extract corresponds to non indexed library\n",
    "    if ext.startswith(\"Blank\"):\n",
    "        date=\"\".join(ext.split(\".\")[1].split(\"-\")[::-1])\n",
    "        if nonIndLib !=\"BL\"+date+\"00\":\n",
    "            print(\"HU? \"+ext+\" for \"+nonIndLib)\n",
    "            break\n",
    "    elif ext+\"00\" != nonIndLib:\n",
    "        print(\"HU? \"+ext+\" for \"+nonIndLib)\n",
    "        print(ext)\n",
    "        break\n",
    "    \n",
    "    extIN.append(extID)\n",
    "    nonIndLibOUT.append(nonIndLibID)\n",
    "    indLibOUT.append(indLibID)\n",
    "\n",
    "print(len(extIN))\n",
    "print(len(nonIndLibOUT))\n",
    "print(len(indLibOUT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17e804d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5423432\n",
      "<Response [204]>\n",
      "5423443\n",
      "<Response [204]>\n",
      "5423433\n",
      "<Response [204]>\n"
     ]
    }
   ],
   "source": [
    "###assign to sampleIN extracts\n",
    "idExp={\"c\":str(value) for key, value in experiments[\"Library Prep. Rascovan Lab protocols\"][\"sampleIN\"].items()}[\"c\"]\n",
    "print(idExp)\n",
    "data=format(extIN)\n",
    "r=requests.put(url+\"/experiments/sections/\"+idExp+\"/samples\",headers=headers1,data = data)\n",
    "print(r)\n",
    "\n",
    "###assign to sampleOUT non indexed Library\n",
    "idExp=experiments[\"Library Prep. Rascovan Lab protocols\"][\"sampleOUT\"][\"UDG-treated extracts\"]\n",
    "print(idExp)\n",
    "data=format(nonIndLibOUT)\n",
    "r=requests.put(url+\"/experiments/sections/\"+format(idExp)+\"/samples\",headers=headers1,data = data)\n",
    "print(r)\n",
    "\n",
    "\n",
    "###assign to sampleOUT indexed Library\n",
    "idExp=experiments[\"Library Prep. Rascovan Lab protocols\"][\"sampleOUT\"][\"Library generated\"]\n",
    "print(idExp)\n",
    "data=format(indLibOUT)\n",
    "r=requests.put(url+\"/experiments/sections/\"+format(idExp)+\"/samples\",headers=headers1,data = data)\n",
    "print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b6fad",
   "metadata": {},
   "source": [
    "### Check if all samples in eLab are assigned to an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cbee7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(name,listToCheck):\n",
    "    return name not in listToCheck\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed69fd",
   "metadata": {},
   "source": [
    "### For Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a26da8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "###check if all individuals as sampleIN for labelling process\n",
    "r=requests.get(url+\"/experiments/sections/\"+format(experiments[\"Labelling process\"][\"sampleIN\"][\"Individuals labelled\"])+\"/samples\",headers=headers2)\n",
    "listInExp=[]\n",
    "for i in r.json().get(\"data\"):\n",
    "    listInExp.append(i[\"name\"])\n",
    "\n",
    "IndnotInLabelling=[element for idx,element in enumerate(registered[\"Individual\"]) if condition(element,listInExp)]\n",
    "print(IndnotInLabelling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9362601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "###check if all Skeleton element  as sampleOUT for labelling process\n",
    "r=requests.get(url+\"/experiments/sections/\"+format(experiments[\"Labelling process\"][\"sampleOUT\"][\"Labelled Skeleton elements \"])+\"/samples\",headers=headers1)\n",
    "listInExp=[]\n",
    "for i in r.json().get(\"data\"):\n",
    "    listInExp.append(i[\"name\"])\n",
    "\n",
    "SkelnotInLabelling=[element for idx,element in enumerate(registered[\"Skeleton Element\"]) if condition(element,listInExp)]\n",
    "print(SkelnotInLabelling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f769e37",
   "metadata": {},
   "source": [
    "### For Drilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fce4805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779 skel element not drilled\n",
      "93 skel element drilled\n"
     ]
    }
   ],
   "source": [
    "listInExp=[]\n",
    "###check if all skeleton element as sampleIN for drilling processes\n",
    "for key in experiments.keys():\n",
    "    if not key.startswith(\"Drilling\"):\n",
    "        continue\n",
    "    for inty in experiments[key][\"sampleIN\"]:\n",
    "        r=requests.get(url+\"/experiments/sections/\"+format(experiments[key][\"sampleIN\"][inty])+\"/samples\",headers=headers1)\n",
    "        if r.status_code!=200:\n",
    "            print(key+\" \"+inty+\" bad request\")\n",
    "            break\n",
    "        for i in r.json().get(\"data\"):\n",
    "            if i[\"name\"] in listInExp:\n",
    "                print(i[\"name\"]+\" assigned to different drilling processes\")\n",
    "            else:\n",
    "                listInExp.append(i[\"name\"])\n",
    "                \n",
    "notDrilled=[element for idx,element in enumerate(registered[\"Skeleton Element\"]) if condition(element,listInExp)]\n",
    "print(format(len(notDrilled))+ \" skel element not drilled\")\n",
    "Drilled=[element for idx,element in enumerate(registered[\"Skeleton Element\"]) if not condition(element,listInExp)]\n",
    "print(format(len(Drilled))+ \" skel element drilled\")\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0189dc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 extracts not assigned to drilling\n",
      "['Blank.2021-07-01', 'Blank.2021-07-08', 'Blank.2021-07-14', 'Blank.2021-05-25']\n",
      "213 extracts assigned to drilling\n"
     ]
    }
   ],
   "source": [
    "listInExp=[]\n",
    "###check if all extracts as sampleOUT for drilling processes\n",
    "for key in experiments.keys():\n",
    "    if not key.startswith(\"Drilling\"):\n",
    "        continue\n",
    "    for inty in experiments[key][\"sampleOUT\"]:\n",
    "        r=requests.get(url+\"/experiments/sections/\"+format(experiments[key][\"sampleOUT\"][inty])+\"/samples\",headers=headers1)\n",
    "        if r.status_code!=200:\n",
    "            print(key+\" \"+inty+\" bad request\")\n",
    "            break\n",
    "        for i in r.json().get(\"data\"):\n",
    "            if i[\"name\"] in listInExp:\n",
    "                print(i[\"name\"]+\" assigned to different drilling processes\")\n",
    "            else:\n",
    "                listInExp.append(i[\"name\"])\n",
    "                \n",
    "notDrilled=[element for idx,element in enumerate(registered[\"Extract\"]) if condition(element,listInExp)]\n",
    "print(format(len(notDrilled))+ \" extracts not assigned to drilling\")\n",
    "print(notDrilled)\n",
    "Drilled=[element for idx,element in enumerate(registered[\"Extract\"]) if not condition(element,listInExp)]\n",
    "print(format(len(Drilled))+ \" extracts assigned to drilling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6809a",
   "metadata": {},
   "source": [
    "### For extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2a79a8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 pulverized pieces not assigned to extraction as IN\n",
      "73 pulverized pieces assigned to extraction as IN\n",
      "4 pulverized pieces assigned to extraction but not Drilling\n",
      "['Blank.2021-07-01', 'Blank.2021-07-08', 'Blank.2021-07-14', 'Blank.2021-05-25']\n"
     ]
    }
   ],
   "source": [
    "listInExp=[]\n",
    "###check if all extracts as sampleIN for extraction\n",
    "for key in experiments.keys():\n",
    "    if not key.startswith(\"Extraction\"):\n",
    "        continue\n",
    "    for inty in experiments[key][\"sampleIN\"]:\n",
    "        r=requests.get(url+\"/experiments/sections/\"+format(experiments[key][\"sampleIN\"][inty])+\"/samples\",headers=headers1)\n",
    "        if r.status_code!=200:\n",
    "            print(key+\" \"+inty+\" bad request\")\n",
    "            break\n",
    "        for i in r.json().get(\"data\"):\n",
    "            if i[\"name\"] in listInExp:\n",
    "                print(i[\"name\"]+\" assigned to different extraction  processes\")\n",
    "            else:\n",
    "                listInExp.append(i[\"name\"])\n",
    "                \n",
    "notExtractedIN=[element for idx,element in enumerate(registered[\"Extract\"]) if condition(element,listInExp)]\n",
    "print(format(len(notExtractedIN))+ \" pulverized pieces not assigned to extraction as IN\")\n",
    "ExtractedIN=[element for idx,element in enumerate(registered[\"Extract\"]) if not condition(element,listInExp)]\n",
    "print(format(len(ExtractedIN))+ \" pulverized pieces assigned to extraction as IN\")\n",
    "\n",
    "#check if all extracted pieces has been drilled\n",
    "ExtractedNotDrilled=[element for idx,element in enumerate(ExtractedIN) if condition(element,Drilled)]\n",
    "print(format(len(ExtractedNotDrilled))+ \" pulverized pieces assigned to extraction but not Drilling\")\n",
    "print(ExtractedNotDrilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2c0bcbd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 pulverized pieces not assigned to extraction as OUT\n",
      "73 pulverized pieces assigned to extraction as OUT\n",
      "0 pulverized pieces assigned to extraction as OUT but not as IN\n",
      "0 pulverized pieces assigned to extraction as IN but not as OUT\n"
     ]
    }
   ],
   "source": [
    "listInExp=[]\n",
    "#check if all extracts (ARXXXX.Y.ZZ) assigned to Extraction as sampleOUT\n",
    "for key in experiments.keys():\n",
    "    if not key.startswith(\"Extraction\"):\n",
    "        continue\n",
    "    for inty in experiments[key][\"sampleOUT\"]:\n",
    "        r=requests.get(url+\"/experiments/sections/\"+format(experiments[key][\"sampleOUT\"][inty])+\"/samples\",headers=headers1)\n",
    "        if r.status_code!=200:\n",
    "            print(key+\" \"+inty+\" bad request\")\n",
    "            break\n",
    "        for i in r.json().get(\"data\"):\n",
    "            if i[\"name\"] in listInExp:\n",
    "                print(i[\"name\"]+\" assigned to different extraction  processes\")\n",
    "            else:\n",
    "                listInExp.append(i[\"name\"])\n",
    "                \n",
    "notExtractedOUT=[element for idx,element in enumerate(registered[\"Extract\"]) if condition(element,listInExp)]\n",
    "print(format(len(notExtractedOUT))+ \" pulverized pieces not assigned to extraction as OUT\")\n",
    "ExtractedOUT=[element for idx,element in enumerate(registered[\"Extract\"]) if not condition(element,listInExp)]\n",
    "print(format(len(ExtractedOUT))+ \" pulverized pieces assigned to extraction as OUT\")\n",
    "\n",
    "#check if all extracted as IN are as OUT\n",
    "ExtractedOUTNotIN=[element for idx,element in enumerate(ExtractedOUT) if condition(element,ExtractedIN)]\n",
    "print(format(len(ExtractedOUTNotIN))+ \" pulverized pieces assigned to extraction as OUT but not as IN\")\n",
    "\n",
    "#check if all extracted as OUT are as IN\n",
    "ExtractedINNotOUT=[element for idx,element in enumerate(ExtractedIN) if condition(element,ExtractedOUT)]\n",
    "print(format(len(ExtractedINNotOUT))+ \" pulverized pieces assigned to extraction as IN but not as OUT\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e92d6",
   "metadata": {},
   "source": [
    "### Library PREP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "289afc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 pulverized pieces not assigned to Lib Prep as IN\n",
      "73 pulverized pieces assigned to Lib Prep as IN\n",
      "0 pulverized pieces assigned to extraction as OUT but not as IN in LibPrep\n"
     ]
    }
   ],
   "source": [
    "listInExp=[]\n",
    "###check if all extracts as sampleIN for library prep\n",
    "for key in experiments.keys():\n",
    "    if not key.startswith(\"Library Prep\"):\n",
    "        continue\n",
    "    for inty in experiments[key][\"sampleIN\"]:\n",
    "        r=requests.get(url+\"/experiments/sections/\"+format(experiments[key][\"sampleIN\"][inty])+\"/samples\",headers=headers1)\n",
    "        if r.status_code!=200:\n",
    "            print(key+\" \"+inty+\" bad request\")\n",
    "            break\n",
    "        for i in r.json().get(\"data\"):\n",
    "            if i[\"name\"] in listInExp:\n",
    "                print(i[\"name\"]+\" assigned to different Lib Prep processes\")\n",
    "            else:\n",
    "                listInExp.append(i[\"name\"])\n",
    "                \n",
    "notLibPrepIN=[element for idx,element in enumerate(registered[\"Extract\"]) if condition(element,listInExp)]\n",
    "print(format(len(notLibPrepIN))+ \" pulverized pieces not assigned to Lib Prep as IN\")\n",
    "LibPrepIN=[element for idx,element in enumerate(registered[\"Extract\"]) if not condition(element,listInExp)]\n",
    "print(format(len(LibPrepIN))+ \" pulverized pieces assigned to Lib Prep as IN\")\n",
    "\n",
    "#check if all extraction as OUT in extract are as IN in LibPrep\n",
    "ExtractedOUTNotLipPrep=[element for idx,element in enumerate(ExtractedOUT) if condition(element,LibPrepIN)]\n",
    "print(format(len(ExtractedOUTNotLipPrep))+ \" pulverized pieces assigned to extraction as OUT but not as IN in LibPrep\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "a3f86adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UDG-treated extracts\n",
      "0 non indexed libraries not assigned to Lib Prep as OUT\n",
      "[]\n",
      "73non indexed libaries assigned to Lib Prep as OUT\n",
      "4 non indexed libraries with no correspondance in IN\n",
      "['BL2505202100', 'BL0107202100', 'BL0807202100', 'BL1407202100']\n",
      "4 indexed libraries with no correspondance for non indexed libraries\n",
      "['Blank.2021-07-0100', 'Blank.2021-07-0800', 'Blank.2021-07-1400', 'Blank.2021-05-2500']\n"
     ]
    }
   ],
   "source": [
    "listInExp=[]\n",
    "###check if all Non Indexed libraries as sampleOUT for library prep\n",
    "for key in experiments.keys():\n",
    "    if not key.startswith(\"Library Prep\"):\n",
    "        continue\n",
    "    for inty in experiments[key][\"sampleOUT\"]:\n",
    "        if not inty.startswith(\"UDG\"):\n",
    "            continue\n",
    "        print(inty)\n",
    "        r=requests.get(url+\"/experiments/sections/\"+format(experiments[key][\"sampleOUT\"][inty])+\"/samples\",headers=headers1)\n",
    "        if r.status_code!=200:\n",
    "            print(key+\" \"+inty+\" bad request\")\n",
    "            break\n",
    "        for i in r.json().get(\"data\"):\n",
    "            if i[\"name\"] in listInExp:\n",
    "                print(i[\"name\"]+\" assigned to different Lib Prep processes\")\n",
    "            else:\n",
    "                listInExp.append(i[\"name\"])\n",
    "            \n",
    "notNiLibPrepOUT=[element for idx,element in enumerate(registered[\"Non Indexed Library\"]) if condition(element,listInExp)]\n",
    "print(format(len(notNiLibPrepOUT))+ \" non indexed libraries not assigned to Lib Prep as OUT\")\n",
    "print(notNiLibPrepOUT)\n",
    "niLibPrepOUT=[element for idx,element in enumerate(registered[\"Non Indexed Library\"]) if not condition(element,listInExp)]\n",
    "print(format(len(niLibPrepOUT))+ \"non indexed libaries assigned to Lib Prep as OUT\")\n",
    "\n",
    "#check if all extraction as IN in extract correspond to non indexed lib as OUT in LibPrep\n",
    "LibPrepINcorres=[s + \"00\" for s in LibPrepIN]\n",
    "niLibPrepOUTNotLibPrepIN=[element for idx,element in enumerate(niLibPrepOUT) if condition(element,LibPrepINcorres)]\n",
    "print(format(len(niLibPrepOUTNotLibPrepIN))+ \" non indexed libraries with no correspondance in IN\")\n",
    "print(niLibPrepOUTNotLibPrepIN)\n",
    "#check if all indexed lib as OUT correspond to non indexed libs as IN in extract in LibPrep\n",
    "niLibPrepINNotLibPrepOUT=[element for idx,element in enumerate(LibPrepINcorres) if  condition(element,niLibPrepOUT)]\n",
    "print(format(len(niLibPrepINNotLibPrepOUT))+ \" indexed libraries with no correspondance for non indexed libraries\")\n",
    "print(niLibPrepINNotLibPrepOUT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5ec2202a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 indexed libraries not assigned to Lib Prep as OUT\n",
      "[]\n",
      "72 indexed libaries assigned to Lib Prep as OUT\n",
      "4  indexed libraries with no correspondance in IN\n",
      "['BL2505202101', 'BL0107202101', 'BL0807202101', 'BL1407202101']\n",
      "5 indexed libraries with no correspondance in IN\n",
      "['Blank.2021-07-0101', 'Blank.2021-07-0801', 'Blank.2021-07-1401', 'AR0026.1.0101', 'Blank.2021-05-2501']\n"
     ]
    }
   ],
   "source": [
    "listInExp=[]\n",
    "###check if all Indexed libraries as sampleOUT for library prep\n",
    "for key in experiments.keys():\n",
    "    if not key.startswith(\"Library Prep\"):\n",
    "        continue\n",
    "    for inty in experiments[key][\"sampleOUT\"]:\n",
    "        if not inty.startswith(\"Library\"):\n",
    "            continue\n",
    "        r=requests.get(url+\"/experiments/sections/\"+format(experiments[key][\"sampleOUT\"][inty])+\"/samples\",headers=headers1)\n",
    "        if r.status_code!=200:\n",
    "            print(key+\" \"+inty+\" bad request\")\n",
    "            break\n",
    "        for i in r.json().get(\"data\"):\n",
    "            if i[\"name\"] in listInExp:\n",
    "                print(i[\"name\"]+\" assigned to different Lib Prep processes\")\n",
    "            else:\n",
    "                listInExp.append(i[\"name\"])\n",
    "            \n",
    "notLibPrepOUT=[element for idx,element in enumerate(registered[\"Indexed Library\"]) if condition(element,listInExp)]\n",
    "print(format(len(notLibPrepOUT))+ \" indexed libraries not assigned to Lib Prep as OUT\")\n",
    "print(notLibPrepOUT)\n",
    "LibPrepOUT=[element for idx,element in enumerate(registered[\"Indexed Library\"]) if not condition(element,listInExp)]\n",
    "print(format(len(LibPrepOUT))+ \" indexed libaries assigned to Lib Prep as OUT\")\n",
    "\n",
    "#check if all extraction as IN in extract correspond to indexed lib as OUT in LibPrep\n",
    "LibPrepINcorres=[s + \"01\" for s in LibPrepIN]\n",
    "LibPrepOUTNotLibPrepIN=[element for idx,element in enumerate(LibPrepOUT) if condition(element,LibPrepINcorres)]\n",
    "print(format(len(LibPrepOUTNotLibPrepIN))+ \"  indexed libraries with no correspondance in IN\")\n",
    "print(LibPrepOUTNotLibPrepIN)\n",
    "#check if all non indexed lib as OUT correspond to extraction as IN in extract in LibPrep\n",
    "LibPrepINNotLibPrepOUT=[element for idx,element in enumerate(LibPrepINcorres) if  condition(element,LibPrepOUT)]\n",
    "print(format(len(LibPrepINNotLibPrepOUT))+ \" indexed libraries with no correspondance in IN\")\n",
    "print(LibPrepINNotLibPrepOUT)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a272b9",
   "metadata": {},
   "source": [
    "## Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ff603d16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nico office\n",
      "In Copenhagen\n",
      "Tom Gilbert Freezer\n",
      "bag A1 + A2\n",
      "calculus extraction\n",
      "petrous back-up\n",
      "petrous extraction\n",
      "pulp back-up\n",
      "pulp extraction\n",
      "root back-up\n",
      "root extraction\n",
      "C group sensitive, blue box, back-up\n",
      "bag Mariano Del Papa calculus\n",
      "Mariano Del Papa calculus extraction\n",
      "already processed\n",
      "Freezer n9\n",
      "Miren Drawer 2\n",
      "Blue Rack 1\n",
      "Freezer 4\n",
      "drawer 1\n",
      "samplebox 1\n",
      "drawer 5\n",
      "extract box 1\n",
      "UDG-SCR libraries no... 2\n",
      "Unknown\n",
      "Individual\n",
      "Site\n",
      "Sequencing\n",
      "Hannes Freezer\n",
      "bag A1 + A2\n",
      "calculus extraction\n",
      "petrous back-up\n",
      "petrous extraction\n",
      "pulp back-up\n",
      "pulp extraction\n",
      "root back-up\n",
      "root extraction\n",
      "C group sensitive, blue box, back-up\n",
      "already processed\n",
      "bag Mariano Del Papa calculus\n",
      "Mariano Del Papa calculus extraction\n",
      "bag B1 + B2\n",
      "pulp back-up\n",
      "pulp extraction\n",
      "calculus extraction\n",
      "petrous extraction\n",
      "petrous back-up\n",
      "root extraction\n",
      "root back-up\n",
      "C group sensitive, blue box, back-up\n",
      "already processed\n",
      "{'Nico office': 774657, 'In Copenhagen': 774658, 'Tom Gilbert Freezer': 774659, 'Tom Gilbert Freezer, bag A1 + A2': 774671, 'Tom Gilbert Freezer, bag A1 + A2, calculus extraction': 774677, 'Tom Gilbert Freezer, bag A1 + A2, petrous back-up': 774678, 'Tom Gilbert Freezer, bag A1 + A2, petrous extraction': 774679, 'Tom Gilbert Freezer, bag A1 + A2, pulp back-up': 774680, 'Tom Gilbert Freezer, bag A1 + A2, pulp extraction': 774681, 'Tom Gilbert Freezer, bag A1 + A2, root back-up': 774682, 'Tom Gilbert Freezer, bag A1 + A2, root extraction': 774683, 'Tom Gilbert Freezer, bag A1 + A2, C group sensitive, blue box, back-up': 774684, 'Tom Gilbert Freezer, bag Mariano Del Papa calculus': 775907, 'Tom Gilbert Freezer, bag Mariano Del Papa calculus, Mariano Del Papa calculus extraction': 775908, 'Tom Gilbert Freezer, bag A1 + A2, already processed': 775909, 'Freezer n9': 774661, 'Freezer n9, Miren Drawer 2': 774674, 'Freezer n9, Miren Drawer 2, Blue Rack 1': 774675, 'Freezer 4': 774836, 'Freezer 4, drawer 1': 774837, 'Freezer 4, drawer 1, samplebox 1': 774838, 'Freezer 4, drawer 5': 775844, 'Freezer 4, drawer 5, extract box 1': 775845, 'Freezer 4, drawer 5, UDG-SCR libraries no... 2': 775860, 'Unknown': 774839, 'Individual': 774998, 'Site': 774999, 'Sequencing': 775861, 'Hannes Freezer': 775975, 'Hannes Freezer, bag A1 + A2': 775976, 'Hannes Freezer, bag A1 + A2, calculus extraction': 775977, 'Hannes Freezer, bag A1 + A2, petrous back-up': 775978, 'Hannes Freezer, bag A1 + A2, petrous extraction': 775979, 'Hannes Freezer, bag A1 + A2, pulp back-up': 775980, 'Hannes Freezer, bag A1 + A2, pulp extraction': 775981, 'Hannes Freezer, bag A1 + A2, root back-up': 775982, 'Hannes Freezer, bag A1 + A2, root extraction': 775983, 'Hannes Freezer, bag A1 + A2, C group sensitive, blue box, back-up': 775984, 'Hannes Freezer, bag A1 + A2, already processed': 775985, 'Hannes Freezer, bag Mariano Del Papa calculus': 775986, 'Hannes Freezer, bag Mariano Del Papa calculus, Mariano Del Papa calculus extraction': 775987, 'Hannes Freezer, bag B1 + B2': 775988, 'Hannes Freezer, bag B1 + B2, pulp back-up': 775989, 'Hannes Freezer, bag B1 + B2, pulp extraction': 775990, 'Hannes Freezer, bag B1 + B2, calculus extraction': 775992, 'Hannes Freezer, bag B1 + B2, petrous extraction': 775993, 'Hannes Freezer, bag B1 + B2, petrous back-up': 775994, 'Hannes Freezer, bag B1 + B2, root extraction': 775996, 'Hannes Freezer, bag B1 + B2, root back-up': 775997, 'Hannes Freezer, bag B1 + B2, C group sensitive, blue box, back-up': 775998, 'Hannes Freezer, bag B1 + B2, already processed': 776000}\n"
     ]
    }
   ],
   "source": [
    "storageByID={}\n",
    "r=requests.get(url+\"/storageLayers\",headers=headers1)\n",
    "stoData=r.json().get(\"data\")\n",
    "for sto in stoData:\n",
    "    storageByID[sto[\"storageLayerID\"]]={\"name\":sto[\"name\"],\"parentID\":sto[\"parentStorageLayerID\"]}\n",
    "    print(sto[\"name\"])\n",
    "def getParentSto(ID,stoDict):\n",
    "    if stoDict[ID][\"parentID\"]==0:\n",
    "        return(stoDict[ID][\"name\"])\n",
    "    else:\n",
    "        return(getParentSto(stoDict[ID][\"parentID\"],stoDict)+\", \"+stoDict[ID][\"name\"])\n",
    "    \n",
    "storage={}\n",
    "for stoID in storageByID.keys():\n",
    "    name=getParentSto(stoID,storageByID)\n",
    "    storage[name]=stoID\n",
    "    \n",
    "print(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadaa1c0",
   "metadata": {},
   "source": [
    "### Assign Individual to Individual artefactual Storage Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "48458ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774999\n",
      "774998\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "for StoType in [\"Site\",\"Individual\"]:\n",
    "    IDsto=format(storage[StoType])\n",
    "    print(IDsto)\n",
    "    for key,id in registered[StoType].items():\n",
    "        r=requests.post(url+\"/samples/moveToLayer/\"+IDsto+\"?sampleIDs=\"+id,headers=headers1,data={})\n",
    "        if r.status_code != 204:\n",
    "            print(\"error for \"+key+\" \"+id)\n",
    "\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e267ef66",
   "metadata": {},
   "source": [
    "### assign Skeleton Element to some locations\n",
    "- those for which batch is A1, A2, B1, B2 or C are assigned to Copenhagen\n",
    "- those for which batch is Sequenced are assigned to Unkown\n",
    "- all the others are assigned to Nico office\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "01a1d61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1                          40\n",
      "A1                          35\n",
      "Sequenced                   21\n",
      "B2                          15\n",
      "C                           11\n",
      "Ready for DNA extraction    11\n",
      "A2                           9\n",
      "Name: 1st Batch, dtype: int64\n",
      "{'Nico office': 774657, 'In Copenhagen': 774658, 'Tom Gilbert Freezer': 774659, 'Tom Gilbert Freezer, bag A1 + A2': 774671, 'Tom Gilbert Freezer, bag A1 + A2, calculus extraction': 774677, 'Tom Gilbert Freezer, bag A1 + A2, petrous back-up': 774678, 'Tom Gilbert Freezer, bag A1 + A2, petrous extraction': 774679, 'Tom Gilbert Freezer, bag A1 + A2, pulp back-up': 774680, 'Tom Gilbert Freezer, bag A1 + A2, pulp extraction': 774681, 'Tom Gilbert Freezer, bag A1 + A2, root back-up': 774682, 'Tom Gilbert Freezer, bag A1 + A2, root extraction': 774683, 'Tom Gilbert Freezer, bag A1 + A2, C group sensitive, blue box, back-up': 774684, 'Tom Gilbert Freezer, bag Mariano Del Papa calculus': 775907, 'Tom Gilbert Freezer, bag Mariano Del Papa calculus, Mariano Del Papa calculus extraction': 775908, 'Tom Gilbert Freezer, bag A1 + A2, already processed': 775909, 'Freezer n9': 774661, 'Freezer n9, Miren Drawer 2': 774674, 'Freezer n9, Miren Drawer 2, Blue Rack 1': 774675, 'Freezer 4': 774836, 'Freezer 4, drawer 1': 774837, 'Freezer 4, drawer 1, samplebox 1': 774838, 'Freezer 4, drawer 5': 775844, 'Freezer 4, drawer 5, extract box 1': 775845, 'Freezer 4, drawer 5, UDG-SCR libraries no... 2': 775860, 'Unknown': 774839, 'Individual': 774998, 'Site': 774999, 'Sequencing': 775861}\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "print(table[\"1st Batch\"].value_counts())\n",
    "print(storage)\n",
    "for item,name in table[\"RascovanLabID\"].items():\n",
    "    id=registered[\"Skeleton Element\"][name]\n",
    "    if table[\"1st Batch\"][item] in [\"A1\",\"A2\",\"B1\",\"B2\",\"C\",\"Ready for DNA extraction\"]:\n",
    "        StoType=\"In Copenhagen\"\n",
    "    elif table[\"1st Batch\"][item] in [\"Sequenced\"]:\n",
    "        StoType=\"Unknown\"\n",
    "    elif format(table[\"1st Batch\"][item]) == \"nan\":\n",
    "        StoType=\"Nico office\"\n",
    "    else:\n",
    "        print(\"not recognized condition \"+format(table[\"1st Batch\"][item]))\n",
    "        break\n",
    "    IDsto=format(storage[StoType])\n",
    "    r=requests.post(url+\"/samples/moveToLayer/\"+IDsto+\"?sampleIDs=\"+id,headers=headers1,data={})\n",
    "    if r.status_code != 204:\n",
    "            print(\"error for \"+name+\" (\"+id+\")\")\n",
    "    \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c97825c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for index,name in extractTable[ExeDict['Name']].items():\n",
    "    idEx=registered[\"Extract\"][name]\n",
    "    r=requests.get(url+\"/samples/get?sampleID=\"+idEx,headers=headers2)\n",
    "    if r.status_code != 200:\n",
    "            print(\"error GET for \"+name+\" (\"+idEx+\")\")\n",
    "            print(r.raise_for_status())\n",
    "    storedIn=r.json()[0][\"storageLayerID\"]\n",
    "    if storedIn != 0:\n",
    "        #print(name+\" already in some storage\")\n",
    "        continue\n",
    "    else:\n",
    "        freezer=extractTable[\"Freezer\"][index]\n",
    "        if format(freezer) in [\"To be spotted\",\"nan\"] :\n",
    "            freezer=\"Unknown\"\n",
    "        freezer=freezer.replace(\"Mariano Del Papa calculus to extract\",\"Mariano Del Papa calculus extraction\")\n",
    "        freezer=freezer.replace(\"A1+A2\",\"A1 + A2\")\n",
    "        freezer=freezer.replace(\"B1+B2\",\"B1 + B2\")\n",
    "        freezer=freezer.replace(\"sub-bag B1+B2 \",\"\")\n",
    "        freezer=freezer.replace(\"sub-bag B1 + B2 \",\"\")\n",
    "        freezer=freezer.replace(\"sub-bag \",\"\")\n",
    "        freezer=freezer.replace(\"pulps\",\"pulp\")\n",
    "        freezer=freezer.replace(\"roots\",\"root\")\n",
    "        freezer=freezer.replace(\" for back-up\",\" back-up\")\n",
    "        freezer=freezer.replace(\" for extraction\",\" extraction\")\n",
    "        freezer=freezer.replace(\" to extract\",\" extraction\")\n",
    "        freezer=freezer.replace(\"freezer\",\"Freezer\")\n",
    "        freezer=freezer.replace(\"Thomas\",\"Tom\")\n",
    "        freezer=freezer.replace(\"Hannes'\",\"Hannes\")\n",
    "        freezer=freezer.replace(\"Miren drawer\",\"Miren Drawer 2\")\n",
    "        freezer=freezer.replace(\"blue rack\",\"Blue Rack 1\")\n",
    "        freezer=freezer.replace(\", front extraction clean room 159\",\"\")\n",
    "        freezer=freezer.replace(\"bag C group sensitive, blue box (back-up)\",\"bag A1 + A2, C group sensitive, blue box, back-up\")\n",
    "        if freezer not in storage:\n",
    "            print(freezer+\" not registsred in eLab\")\n",
    "            break\n",
    "        IDsto=format(storage[freezer])\n",
    "        r=requests.post(url+\"/samples/moveToLayer/\"+IDsto+\"?sampleIDs=\"+idEx,headers=headers1,data={})\n",
    "        if r.status_code != 204:\n",
    "            print(\"error POST for \"+name+\" (\"+idEx+\")\")\n",
    "            print(r.raise_for_status())\n",
    "\n",
    "\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14ea98",
   "metadata": {},
   "source": [
    "### Check samples without storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ea0d1e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual\n",
      "Site\n",
      "Skeleton Element\n",
      "Extract\n",
      "Blank.2021-07-01 NO storage\n",
      "Blank.2021-07-08 NO storage\n",
      "Blank.2021-07-14 NO storage\n",
      "Indexed Library\n",
      "Library pool\n",
      "Non Indexed Library\n"
     ]
    }
   ],
   "source": [
    "for type in registered:\n",
    "    print(type)\n",
    "    for name in registered[type]:\n",
    "        idTY=registered[type][name]\n",
    "        r=requests.get(url+\"/samples/get?sampleID=\"+idTY,headers=headers2)\n",
    "        if r.status_code != 200:\n",
    "            print(\"error GET for \"+name+\" (\"+idTY+\")\")\n",
    "            print(r.raise_for_status())\n",
    "        storedIn=r.json()[0][\"storageLayerID\"]\n",
    "        if storedIn == 0:\n",
    "            print(name+\" NO storage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b07d4",
   "metadata": {},
   "source": [
    "## Some Play Around with the Data Base\n",
    "### get ends points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "99745dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursiveChildren(name,level):\n",
    "    levelSeq=[\"Site\",\"Individual\",\"Skeleton Element\",\n",
    "              \"Extract\",\"Non Indexed Library\",\"Indexed Library\",\"Library Pool\"]\n",
    "    levelName=levelSeq[level]\n",
    "    id=registered[levelName][name]\n",
    "    r=requests.get(url+\"/samples/\"+id+\"/children\",headers=headers2)\n",
    "    json=r.json()\n",
    "    num=json.get(\"recordCount\")\n",
    "    print(''.join([char*level for char in \"\\t\"])+\"- \"+\n",
    "          levelName+\" \\\"\"+name+\"\\\": \"+format(num)+\" \"+levelSeq[level+1])\n",
    "    if num > 0:\n",
    "        level=level+1\n",
    "        data=json.get(\"data\")\n",
    "        ch=-1\n",
    "        while ch < (num-1):\n",
    "            ch+=1\n",
    "            nameCh=data[ch][\"name\"]\n",
    "            recursiveChildren(nameCh,level)\n",
    "    else:\n",
    "        level-=level\n",
    "    if level<0:\n",
    "        return(None)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "235b0e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Site \"El Alto\": 1 Individual\n",
      "\t- Individual \"AR0455\": 1 Skeleton Element\n",
      "\t\t- Skeleton Element \"AR0455.1\": 0 Extract\n",
      "Other Test\n",
      "\t- Individual \"AR0025\": 1 Skeleton Element\n",
      "\t\t- Skeleton Element \"AR0025.1\": 2 Extract\n",
      "\t\t\t- Extract \"AR0025.1.01\": 1 Non Indexed Library\n",
      "\t\t\t\t- Non Indexed Library \"AR0025.1.0100\": 1 Indexed Library\n",
      "\t\t\t\t\t- Indexed Library \"AR0025.1.0101\": 0 Library Pool\n",
      "\t\t\t- Extract \"AR0025.1.02\": 0 Non Indexed Library\n"
     ]
    }
   ],
   "source": [
    "Site=\"El Alto\"\n",
    "id=registered[\"Site\"][Site]\n",
    "recursiveChildren(Site,0)\n",
    "\n",
    "print(\"Other Test\")\n",
    "Indi=\"AR0025\"\n",
    "id=registered[\"Individual\"][Indi]\n",
    "recursiveChildren(Indi,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "b392fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursiveParent(name,level):\n",
    "    levelSeq=['Library Pool', 'Indexed Library', 'Non Indexed Library', 'Extract',\n",
    "              'Skeleton Element', 'Individual', 'Site']\n",
    "    levelName=levelSeq[level]\n",
    "    id=registered[levelName][name]\n",
    "    r=requests.get(url+\"/samples/\"+id+\"/parent\",headers=headers2)\n",
    "    json=r.json()\n",
    "    nameP=json.get(\"name\")\n",
    "    print(name+\" (\"+levelName+\")\\n|\\nV\")\n",
    "    if nameP is None:\n",
    "        print(\"Endpoint\")\n",
    "        return(None)\n",
    "    else:\n",
    "        level=level+1\n",
    "        recursiveParent(nameP,level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "d2a9fb1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR0019.1.0101 (Indexed Library)\n",
      "|\n",
      "V\n",
      "AR0019.1.0100 (Non Indexed Library)\n",
      "|\n",
      "V\n",
      "AR0019.1.01 (Extract)\n",
      "|\n",
      "V\n",
      "AR0019.1 (Skeleton Element)\n",
      "|\n",
      "V\n",
      "AR0019 (Individual)\n",
      "|\n",
      "V\n",
      "Valle inferior río Chubut (Site)\n",
      "|\n",
      "V\n",
      "Endpoint\n"
     ]
    }
   ],
   "source": [
    "recursiveParent(\"AR0019.1.0101\",1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801af74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
