{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a98af6c",
   "metadata": {},
   "source": [
    "# A notebook to move samples into a location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826af25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### a two columns file with col1: sampleID, col2: storage location\n",
    "CSVfilename=\"/Users/pierrespc/Documents/PostDocPasteur/aDNA/Import_eLAB/API_FUNCTIONALITIES/UploadData/assignStorage/Tables/1stBatchCopenhagen.tsv\"\n",
    "delimiterFile=\"\\t\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03927a",
   "metadata": {},
   "source": [
    "## Preparing the note book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4224ba3",
   "metadata": {},
   "source": [
    "Please enter the one-line file where your token is saved in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9b181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenFile=\"/Users/pierrespc/Documents/PostDocPasteur/aDNA/Import_eLAB/API_FUNCTIONALITIES/credentials/tokenELAB\"\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import pandas\n",
    "import numpy\n",
    "from apiclient import discovery, errors\n",
    "from httplib2 import Http\n",
    "from oauth2client import client, file, tools\n",
    "import os.path\n",
    "\n",
    "token = format(open(tokenFile,\"r\").readline().strip())\n",
    "url = \"https://elab-dev.pasteur.fr/api/v1/\"\n",
    "headers1 = {'Authorization': token, 'Accept': 'application/json','Content-Type':'application/json'}\n",
    "headers2 = {'Authorization': token, 'Accept': 'application/json'}\n",
    "\n",
    "\n",
    "\n",
    "def BadRequest(myReq,code=200):\n",
    "    return(myReq.status_code !=code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c61a9fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nico office': 774657, 'In Copenhagen': 774658, 'Tom Gilbert Freezer': 774659, 'Tom Gilbert Freezer, bag A1 + A2': 774671, 'Tom Gilbert Freezer, bag A1 + A2, calculus extraction': 774677, 'Tom Gilbert Freezer, bag A1 + A2, petrous back-up': 774678, 'Tom Gilbert Freezer, bag A1 + A2, petrous extraction': 774679, 'Tom Gilbert Freezer, bag A1 + A2, pulp back-up': 774680, 'Tom Gilbert Freezer, bag A1 + A2, pulp extraction': 774681, 'Tom Gilbert Freezer, bag A1 + A2, root back-up': 774682, 'Tom Gilbert Freezer, bag A1 + A2, root extraction': 774683, 'Tom Gilbert Freezer, bag A1 + A2, C group sensitive, blue box, back-up': 774684, 'Tom Gilbert Freezer, bag Mariano Del Papa calculus': 775907, 'Tom Gilbert Freezer, bag Mariano Del Papa calculus, Mariano Del Papa calculus extraction': 775908, 'Tom Gilbert Freezer, bag A1 + A2, already processed': 775909, 'Freezer n9': 774661, 'Freezer n9, Miren Drawer 2': 774674, 'Freezer n9, Miren Drawer 2, Blue Rack 1': 774675, 'Freezer 4': 774836, 'Freezer 4, drawer 1': 774837, 'Freezer 4, drawer 1, samplebox 1': 774838, 'Freezer 4, drawer 5': 775844, 'Freezer 4, drawer 5, extract box 1': 775845, 'Freezer 4, drawer 5, UDG-SCR libraries no... 2': 775860, 'Unknown': 774839, 'Individual': 774998, 'Site': 774999, 'Sequencing': 775861, 'Hannes Freezer': 775975, 'Hannes Freezer, bag A1 + A2': 775976, 'Hannes Freezer, bag A1 + A2, calculus extraction': 775977, 'Hannes Freezer, bag A1 + A2, petrous back-up': 775978, 'Hannes Freezer, bag A1 + A2, petrous extraction': 775979, 'Hannes Freezer, bag A1 + A2, pulp back-up': 775980, 'Hannes Freezer, bag A1 + A2, pulp extraction': 775981, 'Hannes Freezer, bag A1 + A2, root back-up': 775982, 'Hannes Freezer, bag A1 + A2, root extraction': 775983, 'Hannes Freezer, bag A1 + A2, C group sensitive, blue box, back-up': 775984, 'Hannes Freezer, bag A1 + A2, already processed': 775985, 'Hannes Freezer, bag Mariano Del Papa calculus': 775986, 'Hannes Freezer, bag Mariano Del Papa calculus, Mariano Del Papa calculus extraction': 775987, 'Hannes Freezer, bag B1 + B2': 775988, 'Hannes Freezer, bag B1 + B2, pulp back-up': 775989, 'Hannes Freezer, bag B1 + B2, pulp extraction': 775990, 'Hannes Freezer, bag B1 + B2, calculus extraction': 775992, 'Hannes Freezer, bag B1 + B2, petrous extraction': 775993, 'Hannes Freezer, bag B1 + B2, petrous back-up': 775994, 'Hannes Freezer, bag B1 + B2, root extraction': 775996, 'Hannes Freezer, bag B1 + B2, root back-up': 775997, 'Hannes Freezer, bag B1 + B2, C group sensitive, blue box, back-up': 775998, 'Hannes Freezer, bag B1 + B2, already processed': 776000, 'Hannes Freezer, bag Mix batch': 776116, 'Hannes Freezer, bag Mix batch, mix batch del papa guraeib calculus extraction': 776117, 'Hannes Freezer, bag Mix batch, mix batch del papa guraeib root extraction': 776118, 'Hannes Freezer, bag Mix batch, mix batch monica beron calculus extraction': 776119, 'Hannes Freezer, bag Mix batch, mix batch monica beron pulp extraction': 776120, 'Hannes Freezer, bag Mix batch, mix batch monica beron root extraction': 776121, 'Hannes Freezer, bag Mix batch, mix batch ramiro barberena calculus extraction': 776122, 'Hannes Freezer, bag Mix batch, mix batch ramiro barberena pulp back-up extraction': 776123, 'Hannes Freezer, bag Mix batch, mix batch ramiro barberena pulp extraction': 776124, 'Hannes Freezer, bag Mix batch, mix batch ramiro barberena root extraction': 776125, 'Hannes Freezer, bag Mix batch, mix batch neme san rafael root extraction': 776177, 'Hannes Freezer, bag Mix batch, mix batch neme san rafael pulp extraction': 776178, 'Hannes Freezer, bag Mix batch, mix batch neme san rafael calculus extraction': 776179, 'Hannes Freezer, bag A1 + A2, A1 re-sampled root apex extraction': 776215, 'Hannes Freezer, bag Mix batch, mix batch Peralta Malargue pulp extraction': 776216, 'Hannes Freezer, bag Mix batch, mix batch Peralta Malargue root extraction': 776217, 'Hannes Freezer, bag Mix batch, mix batch Peralta San Juan calculus extraction': 776218, 'Hannes Freezer, bag Mix batch, mix batch Peralta San Juan root extraction': 776219, 'Hannes Freezer, bag Mix batch, mix batch Peralta San Juan pulp extraction': 776220, 'In Tartu': 776098, 'With Lumila': 776238}\n"
     ]
    }
   ],
   "source": [
    "storageByID={}\n",
    "r=requests.get(url+\"/storageLayers\",headers=headers1)\n",
    "stoData=r.json().get(\"data\")\n",
    "for sto in stoData:\n",
    "    storageByID[sto[\"storageLayerID\"]]={\"name\":sto[\"name\"],\"parentID\":sto[\"parentStorageLayerID\"]}\n",
    "    #print(sto[\"name\"])\n",
    "def getParentSto(ID,stoDict):\n",
    "    if stoDict[ID][\"parentID\"]==0:\n",
    "        return(stoDict[ID][\"name\"])\n",
    "    else:\n",
    "        return(getParentSto(stoDict[ID][\"parentID\"],stoDict)+\", \"+stoDict[ID][\"name\"])\n",
    "    \n",
    "    \n",
    "storage={}\n",
    "for stoID in storageByID.keys():\n",
    "    name=getParentSto(stoID,storageByID)\n",
    "    storage[name]=stoID\n",
    "    \n",
    "\n",
    "print(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2061fc0",
   "metadata": {},
   "source": [
    "Prepare all the eLab-API keys necessary to down and upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907ada59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Individual': '39466', 'Site': '39468', 'Skeleton Element': '39469', 'Extract': '39470', 'Indexed Library': '39494', 'Library pool': '39495', 'Non Indexed Library': '39556', 'Bone pellet': '39599'}\n"
     ]
    }
   ],
   "source": [
    "def BadRequest(myReq,code=200):\n",
    "    return(myReq.status_code !=code)\n",
    "\n",
    "\n",
    "r = requests.get(url + \"sampleTypes\", headers = headers2)\n",
    "if BadRequest(r,200):\n",
    "    print(\"Bad request\")\n",
    "data = r.json()\n",
    "types = {}\n",
    "for typ in data.get(\"data\"):\n",
    "    types[format(typ.get(\"name\"))] = format(typ.get(\"sampleTypeID\"))\n",
    "\n",
    "print(types)\n",
    "\n",
    "r = requests.get(url + \"sampleTypes/\" + types[\"Extract\"] + \"/meta\", headers = headers2)\n",
    "if BadRequest(r,200):\n",
    "    print(\"Bad request\")\n",
    "data = r.json()\n",
    "FeateLabExe = {}\n",
    "for feat in ['Name','Description','Note','Amount','Unit',\"parentSampleID\"]:\n",
    "    FeateLabExe[feat] = {\"ID\": \"notMeta\"}\n",
    "for feat in data.get(\"data\"):\n",
    "    FeateLabExe[format(feat.get(\"key\"))] = { \"ID\":format(feat.get(\"sampleTypeMetaID\")),\n",
    "                                              \"TYPE\":format(feat.get(\"sampleDataType\"))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde84886",
   "metadata": {},
   "source": [
    "Now we get all registered ID for all types.\n",
    "(I am lazy now to try to find a clever way to process only the types we need downstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ab6f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual --> 39466\n",
      "Site --> 39468\n",
      "Skeleton Element --> 39469\n",
      "Extract --> 39470\n",
      "Indexed Library --> 39494\n",
      "Library pool --> 39495\n",
      "Non Indexed Library --> 39556\n",
      "Bone pellet --> 39599\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "registered = {}\n",
    "for name in types:\n",
    "    ID=types[name]\n",
    "    print(name + \" --> \" + format(ID))\n",
    "    r = requests.get(url + \"samples\" , headers = headers2, params = {'sampleTypeID': ID})\n",
    "    if BadRequest:\n",
    "        r.raise_for_status()\n",
    "    data = r.json()\n",
    "    myList = {}\n",
    "    for sam in data.get(\"data\"):\n",
    "        if format(sam.get(\"name\")) in myList.keys():\n",
    "            print(name + \": \" + sam.get(\"name\") + \" duplicated\")\n",
    "            break\n",
    "        myList[format(sam.get(\"name\"))]=format(sam.get(\"sampleID\"))\n",
    "    registered[name] = myList\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decef09",
   "metadata": {},
   "source": [
    "Read the file with storage specifications and iterize to move sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b22c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR0001.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR0003.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR0005.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR0007.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR0009.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>AR0351.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>AR0352.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>AR0353.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>AR0354.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>AR0355.1</td>\n",
       "      <td>In Copenhagen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID       Location\n",
       "0    AR0001.1  In Copenhagen\n",
       "1    AR0003.1  In Copenhagen\n",
       "2    AR0005.1  In Copenhagen\n",
       "3    AR0007.1  In Copenhagen\n",
       "4    AR0009.1  In Copenhagen\n",
       "..        ...            ...\n",
       "120  AR0351.1  In Copenhagen\n",
       "121  AR0352.1  In Copenhagen\n",
       "122  AR0353.1  In Copenhagen\n",
       "123  AR0354.1  In Copenhagen\n",
       "124  AR0355.1  In Copenhagen\n",
       "\n",
       "[125 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table=pandas.read_csv(CSVfilename,delimiter=delimiterFile)\n",
    "Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83f473c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "patternDict={\"Skeleton Element\":'[A][R][0-9][0-9][0-9][0-9][.][0-9]',\n",
    "         \"Extract\":'[A][R][0-9][0-9][0-9][0-9][.][0-9][.][0-9]'}\n",
    "\n",
    "\n",
    "for index,name in Table[\"ID\"].items():\n",
    "    Loc=Table.loc[index,\"Location\"]\n",
    "    sampType=\"??\"\n",
    "    for typKey,pattern in patternDict.items():\n",
    "        if re.match(pattern,name):\n",
    "            sampType=typKey\n",
    "    if sampType == \"??\":\n",
    "        print(name+\" has not a recognized pattern and is skipped\")\n",
    "    #print(sampType)\n",
    "    if name not in registered[sampType]:\n",
    "        print(name+\" (\"+typKey+\") is not found in eLab and is skipped\")\n",
    "    if Loc not in storage.keys():\n",
    "            print(name+ \" has wriong location (\"+Loc+\") and is skipped\")\n",
    "    idSam=format(registered[sampType][name])\n",
    "    idLoc=format(storage[Loc])\n",
    "    r = requests.post(url + \"samples/\"+idSam+\"/moveToLayer/\"+idLoc, headers = headers2, params = {})\n",
    "    if BadRequest(r,204):\n",
    "        print(name+\" \"+Loc)\n",
    "        r.raise_for_status()\n",
    "        \n",
    "print(\"finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
